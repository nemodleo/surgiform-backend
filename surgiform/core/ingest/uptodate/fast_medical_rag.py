"""
ì´ˆê³ ì† Elasticsearch ì˜ë£Œ RAG ì‹œìŠ¤í…œ
- ì„ë² ë”© ì„ íƒì  ìƒì„± (ê¸°ë³¸ OFF)  
- ë³‘ë ¬ ì²˜ë¦¬
- ë°°ì¹˜ ìµœì í™”
- ê°„ì†Œí™”ëœ ì¸ë±ì‹±
"""

import json
import os
import sys
import glob
import time
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
from elasticsearch import Elasticsearch
from elasticsearch.helpers import parallel_bulk
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from surgiform.core.ingest.uptodate.medical_parser import MedicalDocumentParser
from surgiform.external.es_client import get_es_client
from surgiform.external.openai_client import get_openai_client


class UltraFastMedicalRAG:
    """ì´ˆê³ ì† ì˜ë£Œ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, enable_embeddings=False):
        print("ğŸš€ ULTRA FAST Medical RAG System")
        
        # Elasticsearch
        self.es_client = get_es_client()
        print("âœ… Elasticsearch connected")
        
        # OpenAI (ì„ íƒì )
        self.enable_embeddings = enable_embeddings
        if enable_embeddings:
            try:
                self.openai_client = get_openai_client()
                print("âœ… Embeddings ENABLED")
            except:
                self.openai_client = None
                self.enable_embeddings = False
                print("âš ï¸ Embeddings DISABLED")
        else:
            self.openai_client = None
            print("âš¡ Embeddings OFF (MAX SPEED)")
        
        # ì¸ë±ìŠ¤
        self.indices = {
            "documents": "fast-docs",
            "sentences": "fast-sentences"
        }
        
        # ìµœì í™” ì„¤ì •
        self.batch_size = 3000
        
    def create_indices(self):
        """ìµœì í™”ëœ ì¸ë±ìŠ¤ ìƒì„±"""
        print("ğŸ—ï¸ Creating fast indices...")
        
        settings = {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "refresh_interval": "60s"
        }
        
        # ë¬¸ì„œ ì¸ë±ìŠ¤
        doc_mapping = {
            "mappings": {
                "properties": {
                    "url": {"type": "keyword"},
                    "title": {"type": "text"},
                    "sentence_count": {"type": "short"},
                    "created_at": {"type": "date"}
                }
            },
            "settings": settings
        }
        
        # ë¬¸ì¥ ì¸ë±ìŠ¤
        sentence_props = {
            "sentence_id": {"type": "keyword"},
            "document_url": {"type": "keyword"},
            "text": {"type": "text"},
            "document_title": {"type": "text"},
            "section": {"type": "keyword"},
            "entities": {"type": "keyword"},
            "created_at": {"type": "date"}
        }
        
        if self.enable_embeddings:
            sentence_props["embedding"] = {
                "type": "dense_vector",
                "dims": 1536,
                "similarity": "cosine"
            }
        
        sentence_mapping = {
            "mappings": {"properties": sentence_props},
            "settings": settings
        }
        
        # ì¸ë±ìŠ¤ ìƒì„±
        for name, mapping in [("documents", doc_mapping), ("sentences", sentence_mapping)]:
            index_name = self.indices[name]
            if not self.es_client.indices.exists(index=index_name):
                self.es_client.indices.create(
                    index=index_name,
                    mappings=mapping["mappings"],
                    settings=mapping["settings"]
                )
                print(f"  âœ… Created {index_name}")
            else:
                print(f"  âš¡ {index_name} exists")
    
    def get_embeddings(self, texts):
        """ë°°ì¹˜ ì„ë² ë”© ìƒì„±"""
        if not self.enable_embeddings:
            return [None] * len(texts)
        
        try:
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
            clean_texts = [t[:6000] if len(t) > 6000 else t for t in texts]
            
            response = self.openai_client.embeddings.create(
                model="text-embedding-ada-002",
                input=clean_texts
            )
            return [item.embedding for item in response.data]
        except Exception as e:
            print(f"âŒ Embedding failed: {e}")
            return [None] * len(texts)
    
    def index_document_ultra_fast(self, json_file):
        """ì´ˆê³ ì† ë¬¸ì„œ ì¸ë±ì‹±"""
        try:
            # JSON ë¡œë“œ
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            title = data.get('title', 'Unknown')
            url = data.get('url', f'file://{json_file}')
            content = data.get('content', '')
            
            # íŒŒì‹±
            parser = MedicalDocumentParser(url)
            parser.parse_html(content)
            
            if not parser.sentences:
                return False
            
            # ì•¡ì…˜ ì¤€ë¹„
            actions = []
            
            # 1. ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
            clean_title = title.replace(' - UpToDate', '').strip()
            actions.append({
                "_index": self.indices["documents"],
                "_id": url,
                "_source": {
                    "url": url,
                    "title": clean_title,
                    "sentence_count": len(parser.sentences),
                    "created_at": datetime.now().isoformat()
                }
            })
            
            # 2. ë¬¸ì¥ ë°ì´í„°
            texts = [clean_title]  # ì œëª© ë¨¼ì €
            sentence_data = []
            
            # ì œëª© ë¬¸ì¥
            title_id = f"title_{hashlib.md5(url.encode()).hexdigest()[:8]}"
            sentence_data.append({
                "sentence_id": title_id,
                "document_url": url,
                "text": clean_title,
                "document_title": clean_title,
                "section": "TITLE",
                "entities": self.extract_simple_entities(clean_title),
                "created_at": datetime.now().isoformat()
            })
            
            # ì¼ë°˜ ë¬¸ì¥ (ìµœëŒ€ 150ê°œ)
            for sentence in parser.sentences[:150]:
                texts.append(sentence.text)
                sentence_data.append({
                    "sentence_id": sentence.sentence_id,
                    "document_url": url,
                    "text": sentence.text,
                    "document_title": clean_title,
                    "section": sentence.section[:20] if sentence.section else "",
                    "entities": sentence.medical_entities[:3],
                    "created_at": datetime.now().isoformat()
                })
            
            # 3. ì„ë² ë”© (ì„ íƒì )
            embeddings = self.get_embeddings(texts)
            
            # 4. ë¬¸ì¥ ì•¡ì…˜ ìƒì„±
            for data, embedding in zip(sentence_data, embeddings):
                if embedding:
                    data["embedding"] = embedding
                
                actions.append({
                    "_index": self.indices["sentences"],
                    "_id": data["sentence_id"],
                    "_source": data
                })
            
            # 5. ë°°ì¹˜ ì¸ë±ì‹±
            success_count = 0
            for success, info in parallel_bulk(
                self.es_client, actions, 
                chunk_size=self.batch_size,
                thread_count=2
            ):
                if success:
                    success_count += 1
            
            return success_count > 0
            
        except Exception as e:
            print(f"âŒ Error indexing {Path(json_file).name}: {e}")
            return False
    
    def extract_simple_entities(self, text):
        """ê°„ë‹¨í•œ ì—”í‹°í‹° ì¶”ì¶œ"""
        keywords = ['surgery', 'treatment', 'cancer', 'diagnosis', 'therapy']
        entities = []
        text_lower = text.lower()
        
        for keyword in keywords:
            if keyword in text_lower:
                entities.append(keyword)
        
        return entities[:3]
    
    def batch_index_ultra_fast(self, directory, max_files=None, workers=8):
        """ì´ˆê³ ì† ë°°ì¹˜ ì²˜ë¦¬"""
        print(f"ğŸš€ ULTRA FAST batch processing")
        print(f"âš¡ Workers: {workers}, Embeddings: {'ON' if self.enable_embeddings else 'OFF'}")
        
        # íŒŒì¼ ì°¾ê¸°
        json_files = glob.glob(os.path.join(directory, "*.json"))
        if max_files:
            json_files = json_files[:max_files]
        
        print(f"ğŸ“‚ Found {len(json_files)} files")
        
        # ì¸ë±ìŠ¤ ìƒì„±
        self.create_indices()
        
        # í†µê³„
        successful = []
        failed = []
        start_time = time.time()
        
        # ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=workers) as executor:
            future_to_file = {
                executor.submit(self.index_document_ultra_fast, f): f 
                for f in json_files
            }
            
            # ì§„í–‰ë¥  í‘œì‹œ
            with tqdm(total=len(json_files), desc="âš¡ ULTRA", unit="docs") as pbar:
                for future in as_completed(future_to_file):
                    file_path = future_to_file[future]
                    
                    try:
                        success = future.result(timeout=20)
                        if success:
                            successful.append(file_path)
                        else:
                            failed.append(file_path)
                    except Exception as e:
                        failed.append(file_path)
                    
                    # ì‹¤ì‹œê°„ ì†ë„ ê³„ì‚°
                    elapsed = time.time() - start_time
                    speed = len(successful) / elapsed if elapsed > 0 else 0
                    
                    pbar.update(1)
                    pbar.set_postfix(speed=f"{speed:.1f}/s")
        
        # ìµœì¢… ê²°ê³¼
        total_time = time.time() - start_time
        final_speed = len(successful) / total_time if total_time > 0 else 0
        
        print(f"\nğŸ‰ COMPLETED!")
        print(f"âš¡ SPEED: {final_speed:.2f} documents/second") 
        print(f"âœ… Success: {len(successful)}/{len(json_files)}")
        print(f"â±ï¸ Time: {total_time:.1f}s")
        
        # ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨
        for index in self.indices.values():
            self.es_client.indices.refresh(index=index)
        
        return {
            "success": len(successful),
            "failed": len(failed), 
            "speed": final_speed,
            "time": total_time
        }
    
    def search_fast(self, query, k=10):
        """ì´ˆê³ ì† ê²€ìƒ‰"""
        start_time = time.time()
        
        try:
            response = self.es_client.search(
                index=self.indices["sentences"],
                query={
                    "multi_match": {
                        "query": query,
                        "fields": ["text^2", "document_title", "entities"]
                    }
                },
                _source=["text", "document_title", "section", "entities"],
                size=k
            )
            
            results = []
            for hit in response['hits']['hits']:
                source = hit['_source']
                results.append({
                    "text": source.get("text", ""),
                    "title": source.get("document_title", ""),
                    "section": source.get("section", ""),
                    "entities": source.get("entities", []),
                    "score": hit['_score']
                })
            
            search_time = time.time() - start_time
            
            return {
                "query": query,
                "results": results,
                "count": len(results),
                "time": search_time
            }
            
        except Exception as e:
            return {
                "query": query,
                "results": [],
                "error": str(e),
                "time": time.time() - start_time
            }


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ì´ˆê³ ì† ì˜ë£Œ RAG')
    parser.add_argument('--directory', default="data/uptodate/general-surgery")
    parser.add_argument('--max-files', type=int)
    parser.add_argument('--workers', type=int, default=8)
    parser.add_argument('--embeddings', action='store_true')
    parser.add_argument('--search', action='store_true')
    
    args = parser.parse_args()
    
    try:
        # RAG ì‹œìŠ¤í…œ
        rag = UltraFastMedicalRAG(enable_embeddings=args.embeddings)
        
        # ë°°ì¹˜ ì²˜ë¦¬
        result = rag.batch_index_ultra_fast(
            args.directory,
            max_files=args.max_files,
            workers=args.workers
        )
        
        print(f"\nğŸ¯ FINAL SPEED: {result['speed']:.2f} docs/sec")
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        if args.search:
            print("\nğŸ” Search test:")
            for query in ["cancer", "surgery", "treatment"]:
                res = rag.search_fast(query, k=3)
                print(f"  {query}: {res['count']} results in {res['time']:.3f}s")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        sys.exit(1)
