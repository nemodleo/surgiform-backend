import asyncio
import os
import dotenv
import json
import re
from pathlib import Path
from typing import Set
from urllib.parse import urlparse
from urllib.parse import urlunparse
from urllib.parse import ParseResult
from playwright.async_api import async_playwright

dotenv.load_dotenv()

ID = os.getenv("UPTODATE_ID")
PW = os.getenv("UPTODATE_PW")


def strip_fragment(url: str) -> str:
    """#fragment ì œê±°"""
    pr: ParseResult = urlparse(url)
    pr = pr._replace(fragment="")
    return urlunparse(pr)


def safe_filename(title: str, max_length: int = 200) -> str:
    """ì œëª©ì„ ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜"""
    # HTML íƒœê·¸ ì œê±°
    title = re.sub(r'<[^>]+>', '', title)
    
    # íŒŒì¼ì‹œìŠ¤í…œì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¬¸ìë“¤ ì œê±°/ë³€í™˜
    title = re.sub(r'[<>:"/\\|?*]', '', title)  # Windows ê¸ˆì§€ ë¬¸ì
    title = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', title)  # ì œì–´ ë¬¸ì
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ, ì•ë’¤ ê³µë°± ì œê±°
    title = re.sub(r'\s+', ' ', title).strip()
    
    # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
    title = title.replace(' ', '_')
    
    # ì ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ëª… ë°©ì§€ (ìˆ¨ê¹€ íŒŒì¼)
    if title.startswith('.'):
        title = '_' + title[1:]
    
    # ê¸¸ì´ ì œí•œ
    if len(title) > max_length:
        title = title[:max_length]
    
    # ë¹ˆ ë¬¸ìì—´ ë°©ì§€
    if not title:
        title = "untitled"
    
    return title


def is_allowed_path(path, target_field: str | None = None):
    if not path.startswith('/contents/'):
        return False
    
    # ì œì™¸í•  í‚¤ì›Œë“œë“¤
    exclude_keywords = [
        'image',
        'calculators',
        'whats-new',
        'search',
        'authors-and-editors',
        'practice-changing-updates',
        'pathways',
    ]
    if any(keyword in path for keyword in exclude_keywords):
        return False
    
    # /contents/table-of-contents ë‹¨ë…ì€ ì œì™¸
    if path == '/contents/table-of-contents':
        return False
    
    # /contents/table-of-contents/general-surgery/~ ê°€ ì•„ë‹ˆë©´ ì œì™¸
    if target_field:
        if path.startswith('/contents/table-of-contents/'):
            if not path.startswith(f'/contents/table-of-contents/{target_field}/'):
                return False
    
    # ì¼ë°˜ topic ë§í¬ëŠ” ëª¨ë‘ í—ˆìš©
    return True


def is_internal_contents(url: str, domain: str, target_field: str | None = None) -> bool:
    """UpToDate /contents/ ë‚´ë¶€ ë§í¬ + avoid íŒ¨í„´ ì œì™¸"""
    try:
        pr = urlparse(url)
        if pr.scheme not in {"http", "https"} or pr.netloc != domain:
            return False
        if not pr.path.startswith("/contents/"):
            return False
        return is_allowed_path(pr.path, target_field)
    except Exception:
        return False


# async def login_uptodate_manual(page, id, pw) -> bool:
#     """UpToDate ìˆ˜ë™ ë¡œê·¸ì¸"""
#     try:
#         print("ğŸ” ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
#         await page.goto("https://www.uptodate.com/login", wait_until="networkidle")
#         
#         print("="*60)
#         print("ğŸ“ ìˆ˜ë™ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤!")
#         print("1. ë¸Œë¼ìš°ì €ì—ì„œ ì‚¬ìš©ìëª…ê³¼ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
#         print(f"   ID: {id}")
#         print(f"   PW: {pw}")
#         print("2. ë¡œê·¸ì¸ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”")
#         print("3. 2FAê°€ ìˆë‹¤ë©´ ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”")
#         print("4. ë¡œê·¸ì¸ì´ ì™„ë£Œë˜ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ê³  Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”")
#         print("="*60)
#         
#         # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
#         input("ë¡œê·¸ì¸ ì™„ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
#         return True
#             
#     except Exception as e:
#         print(f"âŒ ë¡œê·¸ì¸ ê³¼ì •ì—ì„œ ì˜¤ë¥˜: {e}")
#         return False


async def process_single_page(
    page, 
    url: str, 
    out_path: Path, 
    visited: Set[str], 
    queue: list[str], 
    domain: str, 
    target_field: str | None,
    visited_lock: asyncio.Lock,
    queue_lock: asyncio.Lock,
    browser_lock: asyncio.Lock,
    browser_context
) -> None:
    """ë‹¨ì¼ í˜ì´ì§€ ì²˜ë¦¬ ì›Œì»¤ í•¨ìˆ˜"""
    try:
        print(f"ğŸŒ {url}")
        await page.goto(url, wait_until="networkidle", timeout=60_000)
        
        # í˜ì´ì§€ ë¡œë“œ í›„ ê²€ìƒ‰ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë˜ì—ˆëŠ”ì§€ í™•ì¸
        current_url = page.url
        if "/contents/search" in current_url or "/login" in current_url:
            print("ğŸ”„ ê²€ìƒ‰ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ê°ì§€ë¨. í˜ì´ì§€ë¥¼ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
            async with browser_lock:
                await page.close()
                page = await browser_context.new_page()
            
            # ì›ë˜ URLë¡œ ë‹¤ì‹œ ì´ë™
            await page.goto(url, wait_until="networkidle", timeout=60_000)
            
    except Exception as e:
        print(f"âš ï¸ load failed: {e}")
        return

    has_topic = await page.evaluate(
        """
        () => {
            // 1ï¸âƒ£ id='topic-title' ë˜ëŠ” 'topicTitle' ì´ ì¡´ì¬í•˜ë©´ ë°”ë¡œ true
            if (document.getElementById('topic-title') ||
                document.getElementById('topicTitle') ||
                document.getElementById('topicOutline') ||
                document.getElementById('topicArticle') ||
                document.getElementById('topicContent')){
                return true;
            }

            // 2ï¸âƒ£ ê¸°ì¡´ íƒìƒ‰ ë¡œì§ ìœ ì§€
            const nav = document.querySelector(
                'main nav, #topic-outline, .topic-content'
            );
            if (!nav) return false;

            return /\\bTopic\\b/i.test(nav.textContent || '');
        }
        """
    )

    if has_topic:
        title = await page.title()
        
        # íŒŒì¼ëª… ìƒì„± ë° íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        filename = safe_filename(title) + ".json"
        file_path = out_path / filename
        
        if file_path.exists():
            print(f"    â­ï¸ ì´ë¯¸ ì¡´ì¬í•¨: {filename}")
            return  # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
        
        print(f"    âœ… {url}")
        try:
            content = await page.locator("#topicContent").inner_html()
        except Exception as e:
            print(f"    âš ï¸ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            content = ""

        data = {
            "url": url,
            "title": title,
            "content": content
        }

        # ê°œë³„ JSON íŒŒì¼ë¡œ ì €ì¥
        try:
            with file_path.open("w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"    ğŸ’¾ ì €ì¥ë¨: {filename}")
        except Exception as e:
            print(f"    âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return  # ë³¸ë¬¸ í˜ì´ì§€ëŠ” ë§í¬ë¥¼ ë” íƒìƒ‰í•˜ì§€ ì•ŠìŒ

    # 2) ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ë‚´ë¶€ ë§í¬ ìˆ˜ì§‘ â†’ í
    try:
        links = await page.eval_on_selector_all(
            'a[href^="/contents/"]',
            'els => els.map(e => e.href)'
        )
        
        new_links = []
        for href in links:
            href = strip_fragment(href)
            if is_internal_contents(href, domain, target_field):
                async with visited_lock:
                    if href not in visited:
                        new_links.append(href)
        
        if new_links:
            async with queue_lock:
                queue.extend(new_links)
                
    except Exception as e:
        print(f"    âš ï¸ ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")


async def crawl_uptodate_streaming(
        out_path: Path,
        base_url: str = "https://www.uptodate.com/contents/table-of-contents/",
        target_field: str | None = None,
        domain: str = "www.uptodate.com",
        headless: bool = True,
        clear_cache: bool = False,
        max_concurrent: int = 5  # ë™ì‹œ ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜
):
    _url = f"{base_url}/{target_field}" if target_field else base_url
    visited: Set[str] = set()
    queue: list[str] = [_url]
    
    # ë™ì‹œì„± ì œì–´ë¥¼ ìœ„í•œ ë½ê³¼ ì„¸ë§ˆí¬ì–´
    visited_lock = asyncio.Lock()
    queue_lock = asyncio.Lock()
    browser_lock = asyncio.Lock()
    semaphore = asyncio.Semaphore(max_concurrent)

    # out_pathë¥¼ ë””ë ‰í† ë¦¬ë¡œ ì²˜ë¦¬
    out_path.mkdir(parents=True, exist_ok=True)

    playwright = await async_playwright().start()
    
    # ğŸ’¾ user_data_dirì— í”„ë¡œí•„ì„ ì €ì¥í•©ë‹ˆë‹¤
    user_data_dir = "./uptodate_user_profile"
    
    # ìºì‹œ ì´ˆê¸°í™” ìš”ì²­ì´ ìˆìœ¼ë©´ í”„ë¡œí•„ ë””ë ‰í† ë¦¬ ì‚­ì œ
    profile_path = Path(user_data_dir)
    if clear_cache and profile_path.exists():
        import shutil
        print(f"ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™”: {user_data_dir} ì‚­ì œ ì¤‘...")
        shutil.rmtree(profile_path)
        print("âœ… ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # í”„ë¡œí•„ ë””ë ‰í† ë¦¬ ìƒíƒœ í™•ì¸
    if profile_path.exists():
        print(f"ğŸ’¾ ê¸°ì¡´ í”„ë¡œí•„ ë””ë ‰í† ë¦¬ ë°œê²¬: {user_data_dir}")
        # í”„ë¡œí•„ ë‚´ìš© í™•ì¸
        profile_files = list(profile_path.glob("*"))
        print(f"   í”„ë¡œí•„ íŒŒì¼ ê°œìˆ˜: {len(profile_files)}")
    else:
        print(f"ğŸ†• ìƒˆë¡œìš´ í”„ë¡œí•„ ë””ë ‰í† ë¦¬ ìƒì„± ì˜ˆì •: {user_data_dir}")
    
    print(f"ğŸš€ ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘... (headless: {headless}, ë™ì‹œì²˜ë¦¬: {max_concurrent})")
    browser_context = await playwright.chromium.launch_persistent_context(
        user_data_dir=user_data_dir,
        headless=headless
    )

    async def worker():
        """ì›Œì»¤ í•¨ìˆ˜: íì—ì„œ URLì„ ê°€ì ¸ì™€ì„œ ì²˜ë¦¬"""
        page = await browser_context.new_page()
        
        try:
            while True:
                # íì—ì„œ URL ê°€ì ¸ì˜¤ê¸°
                async with queue_lock:
                    if not queue:
                        break
                    url = strip_fragment(queue.pop(0))
                
                # ë°©ë¬¸ ì²´í¬
                async with visited_lock:
                    if url in visited:
                        continue
                    visited.add(url)
                
                # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì œí•œ
                async with semaphore:
                    await process_single_page(
                        page, url, out_path, visited, queue, 
                        domain, target_field, visited_lock, queue_lock, 
                        browser_lock, browser_context
                    )
                    
        finally:
            await page.close()

    try:
        print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {_url}")
        
        # ì›Œì»¤ íƒœìŠ¤í¬ë“¤ ìƒì„±
        workers = [asyncio.create_task(worker()) for _ in range(max_concurrent)]
        
        # ëª¨ë“  ì›Œì»¤ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        await asyncio.gather(*workers)

    finally:
        await browser_context.close()
        await playwright.stop()
    
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ â€“ {out_path} ì— {len(visited)}ê°œ í˜ì´ì§€ ì²˜ë¦¬ë¨")


if __name__ == "__main__":
    # ìºì‹œ ì´ˆê¸°í™”ê°€ í•„ìš”í•˜ë©´ clear_cache=Trueë¡œ ì„¤ì •
    asyncio.run(crawl_uptodate_streaming(
        Path("data/uptodate/general-surgery"),  # í´ë” ê²½ë¡œë¡œ ë³€ê²½
        base_url="https://www.uptodate.com/contents/table-of-contents",
        target_field="general-surgery",
        domain="www.uptodate.com",
        headless=False,
        clear_cache=False,  # ìºì‹œ ë¬¸ì œ ì‹œ Trueë¡œ ë³€ê²½
        max_concurrent=3  # ë™ì‹œ ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ (3-5ê°œ ê¶Œì¥)
    ))
