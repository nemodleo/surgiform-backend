import asyncio
import os
import dotenv
import json
import re
import logging
from datetime import datetime
from pathlib import Path
from typing import Set
from urllib.parse import urlparse
from urllib.parse import urlunparse
from urllib.parse import ParseResult
from playwright.async_api import async_playwright
from tqdm.asyncio import tqdm

dotenv.load_dotenv()

ID = os.getenv("UPTODATE_ID")
PW = os.getenv("UPTODATE_PW")


def strip_fragment(url: str) -> str:
    """#fragment ì œê±°"""
    pr: ParseResult = urlparse(url)
    pr = pr._replace(fragment="")
    return urlunparse(pr)


def safe_filename(title: str, max_length: int = 200) -> str:
    """ì œëª©ì„ ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜"""
    # HTML íƒœê·¸ ì œê±°
    title = re.sub(r'<[^>]+>', '', title)
    
    # íŒŒì¼ì‹œìŠ¤í…œì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¬¸ìë“¤ ì œê±°/ë³€í™˜
    title = re.sub(r'[<>:"/\\|?*]', '', title)  # Windows ê¸ˆì§€ ë¬¸ì
    title = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', title)  # ì œì–´ ë¬¸ì
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ, ì•ë’¤ ê³µë°± ì œê±°
    title = re.sub(r'\s+', ' ', title).strip()
    
    # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
    title = title.replace(' ', '_')
    
    # ì ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ëª… ë°©ì§€ (ìˆ¨ê¹€ íŒŒì¼)
    if title.startswith('.'):
        title = '_' + title[1:]
    
    # ê¸¸ì´ ì œí•œ
    if len(title) > max_length:
        title = title[:max_length]
    
    # ë¹ˆ ë¬¸ìì—´ ë°©ì§€
    if not title:
        title = "untitled"
    
    return title


def is_allowed_path(path, target_field: str | None = None):
    if not path.startswith('/contents/'):
        return False
    
    # ì œì™¸í•  í‚¤ì›Œë“œë“¤
    exclude_keywords = [
        'image',
        'calculators',
        'whats-new',
        'search',
        'authors-and-editors',
        'practice-changing-updates',
        'pathways',
    ]
    if any(keyword in path for keyword in exclude_keywords):
        return False
    
    # /contents/table-of-contents ë‹¨ë…ì€ ì œì™¸
    if path == '/contents/table-of-contents':
        return False
    
    # /contents/table-of-contents/general-surgery/~ ê°€ ì•„ë‹ˆë©´ ì œì™¸
    if target_field:
        if path.startswith('/contents/table-of-contents/'):
            if not path.startswith(f'/contents/table-of-contents/{target_field}/'):
                return False
    
    # ì¼ë°˜ topic ë§í¬ëŠ” ëª¨ë‘ í—ˆìš©
    return True


def is_internal_contents(url: str, domain: str, target_field: str | None = None) -> bool:
    """UpToDate /contents/ ë‚´ë¶€ ë§í¬ + avoid íŒ¨í„´ ì œì™¸"""
    try:
        pr = urlparse(url)
        if pr.scheme not in {"http", "https"} or pr.netloc != domain:
            return False
        if not pr.path.startswith("/contents/"):
            return False
        return is_allowed_path(pr.path, target_field)
    except Exception:
        return False


# async def login_uptodate_manual(page, id, pw) -> bool:
#     """UpToDate ìˆ˜ë™ ë¡œê·¸ì¸"""
#     try:
#         print("ğŸ” ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
#         await page.goto("https://www.uptodate.com/login", wait_until="networkidle")
#         
#         print("="*60)
#         print("ğŸ“ ìˆ˜ë™ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤!")
#         print("1. ë¸Œë¼ìš°ì €ì—ì„œ ì‚¬ìš©ìëª…ê³¼ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
#         print(f"   ID: {id}")
#         print(f"   PW: {pw}")
#         print("2. ë¡œê·¸ì¸ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”")
#         print("3. 2FAê°€ ìˆë‹¤ë©´ ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”")
#         print("4. ë¡œê·¸ì¸ì´ ì™„ë£Œë˜ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ê³  Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”")
#         print("="*60)
#         
#         # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
#         input("ë¡œê·¸ì¸ ì™„ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
#         return True
#             
#     except Exception as e:
#         print(f"âŒ ë¡œê·¸ì¸ ê³¼ì •ì—ì„œ ì˜¤ë¥˜: {e}")
#         return False


async def process_single_page(
    page, 
    url: str, 
    out_path: Path, 
    visited: Set[str], 
    queue: list[str], 
    domain: str, 
    target_field: str | None,
    visited_lock: asyncio.Lock,
    queue_lock: asyncio.Lock,
    browser_lock: asyncio.Lock,
    browser_context,
    saved_count_lock: asyncio.Lock,
    saved_pbar: tqdm,
    queue_pbar: tqdm,
    saved_count: dict,  # dictë¡œ ì „ë‹¬í•˜ì—¬ ì°¸ì¡°ë¡œ ìˆ˜ì • ê°€ëŠ¥í•˜ê²Œ í•¨
    logger: logging.Logger,
    worker_id: int,
    playwright_instance,
    user_data_dir: str,
    headless: bool,
    separate_windows: bool,
    browser_contexts: list
) -> tuple[bool, any, any]:  # (ì„±ê³µì—¬ë¶€, ìƒˆë¡œìš´ ì»¨í…ìŠ¤íŠ¸, ìƒˆë¡œìš´ í˜ì´ì§€)
    """ë‹¨ì¼ í˜ì´ì§€ ì²˜ë¦¬ ì›Œì»¤ í•¨ìˆ˜"""
    try:
        logger.info(f"í˜ì´ì§€ ì ‘ì†: {url}")
        await page.goto(url, wait_until="networkidle", timeout=60_000)
        
        # í˜ì´ì§€ ë¡œë“œ í›„ ê²€ìƒ‰ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë˜ì—ˆëŠ”ì§€ í™•ì¸
        current_url = page.url
        if "/contents/search" in current_url or "/login" in current_url:
            logger.warning(f"ì›Œì»¤ {worker_id}: ë¦¬ë‹¤ì´ë ‰íŠ¸ ê°ì§€ë¨. ë¸Œë¼ìš°ì € ì°½ ì¬ì‹œì‘...")
            
            async with browser_lock:
                # í˜„ì¬ í˜ì´ì§€ì™€ ì»¨í…ìŠ¤íŠ¸ ë‹«ê¸°
                await page.close()
                if separate_windows:
                    await browser_context.close()
                    
                    # ìƒˆë¡œìš´ ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                    new_context = await playwright_instance.chromium.launch_persistent_context(
                        user_data_dir=f"{user_data_dir}_{worker_id}",
                        headless=headless
                    )
                    
                    # browser_contexts ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                    browser_contexts[worker_id] = new_context
                    browser_context = new_context
                    logger.info(f"ì›Œì»¤ {worker_id}: ìƒˆë¡œìš´ ë¸Œë¼ìš°ì € ì°½ ìƒì„± ì™„ë£Œ")
                else:
                    # íƒ­ ëª¨ë“œì—ì„œëŠ” ì»¨í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³  í˜ì´ì§€ë§Œ ì¬ìƒì„±
                    pass
                
                # ìƒˆë¡œìš´ í˜ì´ì§€ ìƒì„±
                page = await browser_context.new_page()
            
            # ì›ë˜ URLë¡œ ë‹¤ì‹œ ì´ë™
            await page.goto(url, wait_until="networkidle", timeout=60_000)
            
    except Exception as e:
        logger.error(f"í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False, browser_context, page

    has_topic = await page.evaluate(
        """
        () => {
            // 1ï¸âƒ£ id='topic-title' ë˜ëŠ” 'topicTitle' ì´ ì¡´ì¬í•˜ë©´ ë°”ë¡œ true
            if (document.getElementById('topic-title') ||
                document.getElementById('topicTitle') ||
                document.getElementById('topicOutline') ||
                document.getElementById('topicArticle') ||
                document.getElementById('topicContent')){
                return true;
            }

            // 2ï¸âƒ£ ê¸°ì¡´ íƒìƒ‰ ë¡œì§ ìœ ì§€
            const nav = document.querySelector(
                'main nav, #topic-outline, .topic-content'
            );
            if (!nav) return false;

            return /\\bTopic\\b/i.test(nav.textContent || '');
        }
        """
    )

    if has_topic:
        title = await page.title()
        
        # íŒŒì¼ëª… ìƒì„± ë° íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        filename = safe_filename(title) + ".json"
        file_path = out_path / filename
        
        if file_path.exists():
            logger.info(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ìŠ¤í‚µ: {filename}")
            return False, browser_context, page
        
        logger.info(f"í† í”½ í˜ì´ì§€ ë°œê²¬: {title}")
        try:
            content = await page.locator("#topicContent").inner_html()
        except Exception as e:
            logger.warning(f"ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            content = ""

        data = {
            "url": url,
            "title": title,
            "content": content
        }

        # ê°œë³„ JSON íŒŒì¼ë¡œ ì €ì¥
        try:
            with file_path.open("w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            
            # ì €ì¥ ì„±ê³µì‹œ progress bar ì—…ë°ì´íŠ¸
            async with saved_count_lock:
                saved_count[filename] = 1
                saved_pbar.update(1)
            
            return True, browser_context, page
        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False, browser_context, page
        
        return False, browser_context, page

    # 2) ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ë‚´ë¶€ ë§í¬ ìˆ˜ì§‘ â†’ í
    try:
        links = await page.eval_on_selector_all(
            'a[href^="/contents/"]',
            'els => els.map(e => e.href)'
        )
        
        new_links = []
        for href in links:
            href = strip_fragment(href)
            if is_internal_contents(href, domain, target_field):
                async with visited_lock:
                    if href not in visited:
                        new_links.append(href)
        
        if new_links:
            async with queue_lock:
                queue.extend(new_links)
                # ìƒˆë¡œìš´ ë§í¬ ì¶”ê°€ì‹œ progress bar ì—…ë°ì´íŠ¸
                queue_pbar.update(len(new_links))
            logger.info(f"ìƒˆë¡œìš´ ë§í¬ {len(new_links)}ê°œ ë°œê²¬í•˜ì—¬ íì— ì¶”ê°€")
                
    except Exception as e:
        logger.error(f"ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    return False, browser_context, page


async def crawl_uptodate_streaming(
        out_path: Path,
        base_url: str = "https://www.uptodate.com/contents/table-of-contents/",
        target_field: str | None = None,
        domain: str = "www.uptodate.com",
        headless: bool = True,
        clear_cache: bool = False,
        max_concurrent: int = 5,  # ë™ì‹œ ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜
        separate_windows: bool = False  # Trueë©´ ê°ê° ë³„ë„ ì°½, Falseë©´ íƒ­ìœ¼ë¡œ
):
    _url = f"{base_url}/{target_field}" if target_field else base_url
    visited: Set[str] = set()
    queue: list[str] = [_url]
    
    # out_pathë¥¼ ë””ë ‰í† ë¦¬ë¡œ ì²˜ë¦¬
    out_path.mkdir(parents=True, exist_ok=True)
    
    # ë¡œê±° ì„¤ì •
    log_file = out_path / f"crawler_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logger = setup_logger(log_file)
    logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {_url}")
    
    # ë™ì‹œì„± ì œì–´ë¥¼ ìœ„í•œ ë½ê³¼ ì„¸ë§ˆí¬ì–´
    visited_lock = asyncio.Lock()
    queue_lock = asyncio.Lock()
    browser_lock = asyncio.Lock()
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # í™œì„± ì›Œì»¤ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
    active_workers = 0
    active_workers_lock = asyncio.Lock()
    shutdown_event = asyncio.Event()
    last_activity_time = asyncio.get_event_loop().time()
    activity_lock = asyncio.Lock()
    IDLE_TIMEOUT = 30  # 30ì´ˆ ë™ì•ˆ ëª¨ë“  ì›Œì»¤ê°€ idleì´ë©´ ì¢…ë£Œ
    
    # Progress bar ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
    saved_count = {}
    saved_count_lock = asyncio.Lock()
    
    # Progress bars ì´ˆê¸°í™”
    visited_pbar = tqdm(desc="ğŸ“‹ Visited", unit="pages", position=0, leave=True)
    queue_pbar = tqdm(desc="ğŸ“ Queue", unit="pages", position=1, leave=True)
    saved_pbar = tqdm(desc="ğŸ’¾ Saved", unit="files", position=2, leave=True)
    
    # ì´ˆê¸° í í¬ê¸° ì„¤ì •
    queue_pbar.total = len(queue)
    queue_pbar.update(len(queue))

    playwright = await async_playwright().start()
    
    # ğŸ’¾ user_data_dirì— í”„ë¡œí•„ì„ ì €ì¥í•©ë‹ˆë‹¤
    user_data_dir = "./uptodate_user_profile"
    
    # ìºì‹œ ì´ˆê¸°í™” ìš”ì²­ì´ ìˆìœ¼ë©´ í”„ë¡œí•„ ë””ë ‰í† ë¦¬ ì‚­ì œ
    profile_path = Path(user_data_dir)
    if clear_cache and profile_path.exists():
        import shutil
        logger.info(f"ìºì‹œ ì´ˆê¸°í™”: {user_data_dir} ì‚­ì œ ì¤‘...")
        shutil.rmtree(profile_path)
        logger.info("ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # í”„ë¡œí•„ ë””ë ‰í† ë¦¬ ìƒíƒœ í™•ì¸
    if profile_path.exists():
        logger.info(f"ê¸°ì¡´ í”„ë¡œí•„ ë””ë ‰í† ë¦¬ ë°œê²¬: {user_data_dir}")
        # í”„ë¡œí•„ ë‚´ìš© í™•ì¸
        profile_files = list(profile_path.glob("*"))
        logger.info(f"   í”„ë¡œí•„ íŒŒì¼ ê°œìˆ˜: {len(profile_files)}")
    else:
        logger.info(f"ìƒˆë¡œìš´ í”„ë¡œí•„ ë””ë ‰í† ë¦¬ ìƒì„± ì˜ˆì •: {user_data_dir}")
    
    window_type = "ë³„ë„ ì°½" if separate_windows else "íƒ­"
    logger.info(f"ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘... (headless: {headless}, ë™ì‹œì²˜ë¦¬: {max_concurrent}ê°œ {window_type})")
    
    if separate_windows:
        # ë³„ë„ ì°½ ëª¨ë“œ: ê° ì›Œì»¤ë§ˆë‹¤ ë…ë¦½ëœ ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤
        browser_contexts = []
        for i in range(max_concurrent):
            context = await playwright.chromium.launch_persistent_context(
                user_data_dir=f"{user_data_dir}_{i}",  # ê°ê° ë‹¤ë¥¸ í”„ë¡œí•„
                headless=headless
            )
            browser_contexts.append(context)
    else:
        # íƒ­ ëª¨ë“œ: í•˜ë‚˜ì˜ ë¸Œë¼ìš°ì €ì—ì„œ ì—¬ëŸ¬ íƒ­
        browser_context = await playwright.chromium.launch_persistent_context(
            user_data_dir=user_data_dir,
            headless=headless
        )

    async def worker(worker_id: int = 0):
        """ì›Œì»¤ í•¨ìˆ˜: íì—ì„œ URLì„ ê°€ì ¸ì™€ì„œ ì²˜ë¦¬"""
        nonlocal active_workers, last_activity_time, saved_count
        
        if separate_windows:
            context = browser_contexts[worker_id]
            page = await context.new_page()
            current_context = context
        else:
            page = await browser_context.new_page()
            current_context = browser_context
        
        logger.info(f"ì›Œì»¤ {worker_id} ì‹œì‘")
        
        try:
            while not shutdown_event.is_set():
                # íì—ì„œ URL ê°€ì ¸ì˜¤ê¸°
                url = None
                async with queue_lock:
                    if queue:
                        url = strip_fragment(queue.pop(0))
                        # íì—ì„œ ì œê±°í–ˆìœ¼ë¯€ë¡œ progress bar ì—…ë°ì´íŠ¸
                        queue_pbar.update(-1)
                        # í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸
                        async with activity_lock:
                            last_activity_time = asyncio.get_event_loop().time()
                
                if url is None:
                    # íê°€ ë¹„ì–´ìˆìœ¼ë©´ ì ì‹œ ëŒ€ê¸° (30ì´ˆ íƒ€ì„ì•„ì›ƒì€ ë³„ë„ íƒœìŠ¤í¬ì—ì„œ ì²˜ë¦¬)
                    await asyncio.sleep(2)
                    continue
                
                # ë°©ë¬¸ ì²´í¬
                async with visited_lock:
                    if url in visited:
                        continue
                    visited.add(url)
                    # visitedì— ì¶”ê°€í–ˆìœ¼ë¯€ë¡œ progress bar ì—…ë°ì´íŠ¸
                    visited_pbar.update(1)
                
                # í™œì„± ì›Œì»¤ ìˆ˜ ì¦ê°€ ë° í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸
                async with active_workers_lock:
                    active_workers += 1
                async with activity_lock:
                    last_activity_time = asyncio.get_event_loop().time()
                
                try:
                    # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì œí•œ
                    async with semaphore:
                        # í˜ì´ì§€ ì²˜ë¦¬ ë° ì €ì¥ ì—¬ë¶€ í™•ì¸
                        saved, updated_context, new_page = await process_single_page(
                            page, url, out_path, visited, queue, 
                            domain, target_field, visited_lock, queue_lock, 
                            browser_lock, current_context, saved_count_lock, 
                            saved_pbar, queue_pbar, saved_count, logger,
                            worker_id, playwright, user_data_dir, headless, separate_windows, browser_contexts
                        )
                        
                        # ì»¨í…ìŠ¤íŠ¸ë‚˜ í˜ì´ì§€ê°€ ë³€ê²½ë˜ì—ˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                        if updated_context != current_context:
                            current_context = updated_context
                            logger.info(f"ì›Œì»¤ {worker_id}: ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ë¨")
                        
                        if new_page != page:
                            page = new_page
                            logger.info(f"ì›Œì»¤ {worker_id}: í˜ì´ì§€ ì—…ë°ì´íŠ¸ë¨")
                        
                        # saved_countëŠ” process_single_pageì—ì„œ ì²˜ë¦¬ë¨
                                
                finally:
                    # í™œì„± ì›Œì»¤ ìˆ˜ ê°ì†Œ ë° í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸
                    async with active_workers_lock:
                        active_workers -= 1
                    async with activity_lock:
                        last_activity_time = asyncio.get_event_loop().time()
                    
        except Exception as e:
            logger.error(f"ì›Œì»¤ {worker_id} ì˜¤ë¥˜: {e}")
        finally:
            logger.info(f"ì›Œì»¤ {worker_id} ì¢…ë£Œ")
            await page.close()

    async def timeout_monitor():
        """30ì´ˆ ë™ì•ˆ ëª¨ë“  ì›Œì»¤ê°€ idleì´ë©´ ì¢…ë£Œ ì‹ í˜¸"""
        nonlocal active_workers, last_activity_time
        
        while not shutdown_event.is_set():
            await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì²´í¬
            
            current_time = asyncio.get_event_loop().time()
            
            async with active_workers_lock:
                current_active = active_workers
            
            async with activity_lock:
                time_since_last_activity = current_time - last_activity_time
            
            if current_active == 0 and time_since_last_activity >= IDLE_TIMEOUT:
                logger.info(f"íƒ€ì„ì•„ì›ƒ: {IDLE_TIMEOUT}ì´ˆ ë™ì•ˆ ëª¨ë“  ì›Œì»¤ê°€ idle ìƒíƒœ - ì¢…ë£Œ ì‹ í˜¸ ë°œì†¡")
                shutdown_event.set()
                break

    async def progress_monitor():
        """Progress bar ìƒíƒœ ì—…ë°ì´íŠ¸"""
        while not shutdown_event.is_set():
            await asyncio.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
            
            # í˜„ì¬ ìƒíƒœ ì •ë³´ ìˆ˜ì§‘
            async with visited_lock:
                visited_count = len(visited)
            async with queue_lock:
                queue_count = len(queue)
            async with saved_count_lock:
                current_saved = len(saved_count)
            async with active_workers_lock:
                current_active = active_workers
                
            # Progress bar ì„¤ëª… ì—…ë°ì´íŠ¸
            visited_pbar.set_description(f"ğŸ“‹ Visited ({visited_count})")
            queue_pbar.set_description(f"ğŸ“ Queue ({queue_count}) [Active: {current_active}]")
            saved_pbar.set_description(f"ğŸ’¾ Saved ({current_saved})")
            
            visited_pbar.refresh()
            queue_pbar.refresh()
            saved_pbar.refresh()

    try:
        logger.info("í¬ë¡¤ë§ ì›Œì»¤ë“¤ ì‹œì‘")
        
        # ì›Œì»¤ íƒœìŠ¤í¬ë“¤ ìƒì„±
        if separate_windows:
            workers = [asyncio.create_task(worker(i)) for i in range(max_concurrent)]
        else:
            workers = [asyncio.create_task(worker(i)) for i in range(max_concurrent)]
        
        # ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ë“¤ ì¶”ê°€
        timeout_task = asyncio.create_task(timeout_monitor())
        progress_task = asyncio.create_task(progress_monitor())
        
        # ëª¨ë“  íƒœìŠ¤í¬ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        await asyncio.gather(*workers, timeout_task, progress_task)

    finally:
        # Progress bars ì •ë¦¬
        visited_pbar.close()
        queue_pbar.close()
        saved_pbar.close()
        
        if separate_windows:
            for context in browser_contexts:
                await context.close()
        else:
            await browser_context.close()
        await playwright.stop()
    
    logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ - {out_path} ì— {len(visited)}ê°œ í˜ì´ì§€ ì²˜ë¦¬ë¨, {len(saved_count)}ê°œ íŒŒì¼ ì €ì¥ë¨")
    
    # Progress barsê°€ ë‹«íŒ í›„ì—ë§Œ í™”ë©´ì— ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ë¡œê·¸: {log_file}")
    print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {out_path}")
    print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {len(visited)}ê°œ í˜ì´ì§€ ë°©ë¬¸, {len(saved_count)}ê°œ íŒŒì¼ ì €ì¥")


def setup_logger(log_file: Path) -> logging.Logger:
    """ë¡œê±° ì„¤ì •"""
    logger = logging.getLogger('uptodate_crawler')
    logger.setLevel(logging.INFO)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # í¬ë§·í„° ì„¤ì •
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    return logger


if __name__ == "__main__":
    # ìºì‹œ ì´ˆê¸°í™”ê°€ í•„ìš”í•˜ë©´ clear_cache=Trueë¡œ ì„¤ì •
    asyncio.run(crawl_uptodate_streaming(
        Path("data/uptodate/general-surgery"),  # í´ë” ê²½ë¡œë¡œ ë³€ê²½
        base_url="https://www.uptodate.com/contents/table-of-contents",
        target_field="general-surgery",
        domain="www.uptodate.com",
        headless=True,
        clear_cache=False,  # ìºì‹œ ë¬¸ì œ ì‹œ Trueë¡œ ë³€ê²½
        max_concurrent=5,  # ë™ì‹œ ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ (3-5ê°œ ê¶Œì¥)
        separate_windows=True  # 3ê°œì˜ ë³„ë„ ì°½ì´ ì—´ë¦¼
    ))
