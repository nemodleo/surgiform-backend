"""
ì˜ë£Œ ë¬¸ì„œìš© Graph RAG ì‹œìŠ¤í…œ
Neo4j ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤ì™€ ë²¡í„° ê²€ìƒ‰ì„ ê²°í•©í•œ RAG ì‹œìŠ¤í…œ
"""

import re
import json
import os
import hashlib
from typing import List, Dict, Any
from langchain_community.graphs import Neo4jGraph
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Neo4jVector
from dotenv import load_dotenv

from surgiform.core.ingest.uptodate.medical_parser import MedicalSentence, MedicalSection, MedicalDocumentParser

load_dotenv()

class MedicalGraphRAGBuilder:
    """ì˜ë£Œ ë¬¸ì„œìš© Graph RAG êµ¬ì¶•ê¸°"""
    
    def __init__(self, neo4j_url: str = None, neo4j_username: str = None, 
                 neo4j_password: str = None, openai_api_key: str = None):
        """
        Graph RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            neo4j_url: Neo4j ë°ì´í„°ë² ì´ìŠ¤ URL
            neo4j_username: Neo4j ì‚¬ìš©ìëª…
            neo4j_password: Neo4j ë¹„ë°€ë²ˆí˜¸  
            openai_api_key: OpenAI API í‚¤
        """
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
        self.neo4j_url = neo4j_url or os.getenv('NEO4J_URL', 'bolt://localhost:7687')
        self.neo4j_username = neo4j_username or os.getenv('NEO4J_USERNAME', 'neo4j')
        self.neo4j_password = neo4j_password or os.getenv('NEO4J_PASSWORD', 'password')
        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
        
        if not self.openai_api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable.")
        
        # Neo4j ì—°ê²°
        print(f"ğŸ”— Connecting to Neo4j at {self.neo4j_url}")
        self.graph = Neo4jGraph(
            url=self.neo4j_url,
            username=self.neo4j_username,
            password=self.neo4j_password
        )
        
        # OpenAI ì„ë² ë”© ì´ˆê¸°í™”
        print("ğŸ¤– Initializing OpenAI embeddings")
        self.embeddings = OpenAIEmbeddings(openai_api_key=self.openai_api_key)
        self.vector_store = None
        
        print("âœ… MedicalGraphRAGBuilder initialized")
        
    def create_graph_schema(self):
        """ê·¸ë˜í”„ ìŠ¤í‚¤ë§ˆ ìƒì„± (Neo4j 5 í˜¸í™˜)"""
        print("ğŸ—ï¸ Creating graph schema...")
        
        schema_queries = [
            # ë…¸ë“œ ì œì•½ì¡°ê±´
            "CREATE CONSTRAINT sentence_id IF NOT EXISTS FOR (s:Sentence) REQUIRE s.id IS UNIQUE",
            "CREATE CONSTRAINT section_id IF NOT EXISTS FOR (sec:Section) REQUIRE sec.id IS UNIQUE",
            "CREATE CONSTRAINT reference_id IF NOT EXISTS FOR (r:Reference) REQUIRE r.id IS UNIQUE", 
            "CREATE CONSTRAINT image_id IF NOT EXISTS FOR (i:Image) REQUIRE i.id IS UNIQUE",
            "CREATE CONSTRAINT entity_id IF NOT EXISTS FOR (e:MedicalEntity) REQUIRE e.name IS UNIQUE",
            "CREATE CONSTRAINT document_url IF NOT EXISTS FOR (d:Document) REQUIRE d.url IS UNIQUE",
            
            # ì¼ë°˜ ì¸ë±ìŠ¤
            "CREATE INDEX entity_name IF NOT EXISTS FOR (e:MedicalEntity) ON (e.name)",
            "CREATE INDEX entity_type IF NOT EXISTS FOR (e:MedicalEntity) ON (e.type)",
            "CREATE INDEX section_title IF NOT EXISTS FOR (sec:Section) ON (sec.title)",
        ]
        
        for query in schema_queries:
            try:
                self.graph.query(query)
                print(f"  âœ… {query}")
            except Exception as e:
                print(f"  âš ï¸ Schema warning: {e}")
        
        # ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± (Neo4j 5 ë¬¸ë²•) - ë¨¼ì € ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ í›„ ìƒì„±
        try:
            # ê¸°ì¡´ ë²¡í„° ì¸ë±ìŠ¤ ì‚­ì œ
            drop_vector_index = "DROP INDEX sentence_vec IF EXISTS"
            self.graph.query(drop_vector_index)
            
            # ìƒˆ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
            vector_index_query = """
            CREATE VECTOR INDEX sentence_vec
            FOR (s:Sentence) ON (s.embedding)
            OPTIONS {indexConfig: {`vector.dimensions`: 1536, `vector.similarity_function`: 'cosine'}}
            """
            self.graph.query(vector_index_query)
            print("  âœ… Vector index created")
        except Exception as e:
            print(f"  âš ï¸ Vector index warning: {e}")
            # ë°±ì—… ë°©ë²•ìœ¼ë¡œ ê°„ë‹¨í•œ ì¸ë±ìŠ¤ ìƒì„± ì‹œë„
            try:
                simple_index = "CREATE INDEX sentence_text IF NOT EXISTS FOR (s:Sentence) ON (s.text)"
                self.graph.query(simple_index)
                print("  âœ… Text index created as fallback")
            except Exception as e2:
                print(f"  âš ï¸ Text index warning: {e2}")
    
    def build_graph_from_parser(self, parser: MedicalDocumentParser, document_title: str, document_url: str):
        """íŒŒì„œ ê°ì²´ë¡œë¶€í„° ê·¸ë˜í”„ êµ¬ì¶•"""
        print(f"ğŸš€ Building graph from parsed document: {document_title}")
        
        # 1. ìŠ¤í‚¤ë§ˆ ìƒì„±
        self.create_graph_schema()
        
        # 2. ë¬¸ì„œ ë…¸ë“œ ìƒì„±
        self._create_document_node(document_title, document_url, parser.get_parsing_stats())
        
        # 3. ì„¹ì…˜ ë…¸ë“œ ìƒì„±
        print(f"ğŸ“‘ Creating {len(parser.sections)} section nodes...")
        for section in parser.sections:
            self._create_section_node(section, document_url)
        
        # 4. ë¬¸ì¥ ë…¸ë“œ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
        print(f"ğŸ“ Creating {len(parser.sentences)} sentence nodes...")
        self._create_sentences_batch(parser.sentences, document_url)
        
        # 5. ë²¡í„° ìŠ¤í† ì–´ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        self._create_vector_store()
        
        print(f"âœ… Graph built successfully for: {document_title}")
    
    def build_graph_from_html(self, html_content: str, document_title: str, document_url: str):
        """HTML ë‚´ìš©ìœ¼ë¡œë¶€í„° ì§ì ‘ ê·¸ë˜í”„ êµ¬ì¶•"""
        print(f"ğŸ“„ Parsing HTML document: {document_title}")
        
        # íŒŒì„œ ìƒì„± ë° ì‹¤í–‰
        parser = MedicalDocumentParser(document_url)
        parser.parse_html(html_content)
        
        # íŒŒì„œ ê²°ê³¼ë¡œ ê·¸ë˜í”„ êµ¬ì¶•
        self.build_graph_from_parser(parser, document_title, document_url)
    
    def _create_document_node(self, title: str, url: str, stats: Dict):
        """ë¬¸ì„œ ë…¸ë“œ ìƒì„±"""
        # "- UpToDate" ì œê±°í•œ í•µì‹¬ ì œëª©ë§Œ ì €ì¥
        clean_title = title.replace(' - UpToDate', '').strip()
        
        doc_query = """
        MERGE (d:Document {url: $url})
        SET d.title = $clean_title,
            d.created_at = datetime(),
            d.total_sections = $total_sections,
            d.total_sentences = $total_sentences,
            d.total_references = $total_references,
            d.total_images = $total_images,
            d.total_entities = $total_entities
        RETURN d
        """
        
        params = {
            "url": url,
            "clean_title": clean_title,
            "total_sections": stats.get("total_sections", 0),
            "total_sentences": stats.get("total_sentences", 0),
            "total_references": stats.get("total_references", 0),
            "total_images": stats.get("total_images", 0),
            "total_entities": stats.get("total_entities", 0)
        }
        
        self.graph.query(doc_query, params)
        print(f"  ğŸ“„ Document node created with clean title: {clean_title}")
        
        # Titleì„ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶”ê°€
        self._create_document_title_sentence(clean_title, url)
    
    def _create_document_title_sentence(self, clean_title: str, document_url: str):
        """ë¬¸ì„œ ì œëª©ì„ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„±"""
        print(f"  ğŸ¯ Creating title sentence for: {clean_title}")
        
        try:
            # ì œëª© ì„ë² ë”© ìƒì„±
            title_embedding = self.embeddings.embed_documents([clean_title])[0]
            
            # ì œëª©ì„ íŠ¹ë³„í•œ ë¬¸ì¥ ë…¸ë“œë¡œ ìƒì„±
            title_sentence_query = """
            MATCH (d:Document {url: $doc_url})
            MERGE (title_sent:Sentence {id: $title_sentence_id})
            SET title_sent.text = $title_text,
                title_sent.section_name = "DOCUMENT_TITLE",
                title_sent.is_document_title = true,
                title_sent.embedding = $title_embedding,
                title_sent.created_at = datetime()
            MERGE (d)-[:HAS_TITLE_SENTENCE]->(title_sent)
            MERGE (d)-[:CONTAINS_SENTENCE]->(title_sent)
            RETURN title_sent
            """
            
            # ê³ ìœ í•œ ì œëª© ë¬¸ì¥ ID ìƒì„±
            title_hash = hashlib.md5(f"title_{document_url}".encode()).hexdigest()[:8]
            title_sentence_id = f"title_sent_{title_hash}"
            
            params = {
                "doc_url": document_url,
                "title_sentence_id": title_sentence_id,
                "title_text": clean_title,
                "title_embedding": title_embedding
            }
            
            self.graph.query(title_sentence_query, params)
            print(f"    âœ… Title sentence created with embedding")
            
            # ì œëª©ì—ì„œ ì˜ë£Œ ì—”í‹°í‹° ì¶”ì¶œí•˜ì—¬ ì—°ê²°
            self._extract_title_entities(clean_title, title_sentence_id)
            
        except Exception as e:
            print(f"    âš ï¸ Failed to create title sentence: {e}")
    
    def _extract_title_entities(self, title_text: str, sentence_id: str):
        """ì œëª©ì—ì„œ ì˜ë£Œ ì—”í‹°í‹° ì¶”ì¶œí•˜ê³  ì—°ê²°"""
        try:
            # ê°„ë‹¨í•œ ì˜ë£Œ ì—”í‹°í‹° ì¶”ì¶œ (medical_parserì˜ ë¡œì§ì„ ê°„ì†Œí™”)
            words = title_text.lower().split()
            medical_terms = []
            
            # ì¤‘ìš”í•œ ì˜ë£Œ ìš©ì–´ë“¤ í•„í„°ë§
            for word in words:
                word_clean = word.strip('.,;:-()[]')
                if len(word_clean) > 3 and any(term in word_clean for term in [
                    'cancer', 'tumor', 'carcinoma', 'treatment', 'surgery', 'diagnosis',
                    'therapy', 'disease', 'syndrome', 'injury', 'trauma', 'hypogonadism',
                    'testicular', 'gallstone', 'reflux', 'lung', 'bladder', 'ileus',
                    'endovascular', 'metastases', 'nutrition', 'bowel', 'inflammatory'
                ]):
                    medical_terms.append(word_clean)
            
            # ì˜ë£Œ ì—”í‹°í‹° ë…¸ë“œ ìƒì„±
            for entity in medical_terms:
                self._create_entity_node(entity, sentence_id)
            
            if medical_terms:
                print(f"    ğŸ·ï¸ Extracted {len(medical_terms)} entities from title: {medical_terms}")
                
        except Exception as e:
            print(f"    âš ï¸ Title entity extraction error: {e}")
    
    def _create_section_node(self, section: MedicalSection, document_url: str):
        """ì„¹ì…˜ ë…¸ë“œ ìƒì„±"""
        query = """
        MATCH (d:Document {url: $doc_url})
        MERGE (s:Section {id: $section_id})
        SET s.title = $title,
            s.content = $content,
            s.level = $level,
            s.created_at = datetime()
        MERGE (d)-[:CONTAINS_SECTION]->(s)
        RETURN s
        """
        
        params = {
            "doc_url": document_url,
            "section_id": section.section_id,
            "title": section.title,
            "content": section.content[:5000],  # ë‚´ìš© ê¸¸ì´ ì œí•œ
            "level": section.level
        }
        
        self.graph.query(query, params)
    
    def _create_sentences_batch(self, sentences: List[MedicalSentence], document_url: str):
        """ë¬¸ì¥ë“¤ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„ë² ë”© íš¨ìœ¨ì„± í–¥ìƒ"""
        
        if not sentences:
            print("âš ï¸ No sentences to process")
            return
        
        print(f"ğŸ”„ Processing {len(sentences)} sentences in batches...")
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì • (OpenAI API ì œí•œ ê³ ë ¤)
        batch_size = 50  # ë” ì‘ì€ ë°°ì¹˜ë¡œ ì•ˆì •ì„± í–¥ìƒ
        
        for i in range(0, len(sentences), batch_size):
            batch = sentences[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(sentences) - 1) // batch_size + 1
            
            print(f"  ğŸ“¦ Processing batch {batch_num}/{total_batches} ({len(batch)} sentences)")
            
            # ì°¸ì¡° ë²ˆí˜¸ ì œê±° í›„ ì„ë² ë”© ìƒì„±
            clean_texts = []
            for sentence in batch:
                # ì°¸ì¡° ë²ˆí˜¸ [1], [1,2] ë“± ì œê±°
                clean_text = re.sub(r'\[\d+(?:,\d+)*\]', '', sentence.text).strip()
                # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ìë¥´ê¸° (í† í° ì œí•œ)
                if len(clean_text) > 8000:
                    clean_text = clean_text[:8000] + "..."
                clean_texts.append(clean_text)
            
            # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
            try:
                embeddings = self.embeddings.embed_documents(clean_texts)
                print(f"    âœ… Generated {len(embeddings)} embeddings")
            except Exception as e:
                print(f"    âŒ Embedding error: {e}")
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ìŠ¤í‚µí•˜ê³  ê³„ì† ì§„í–‰
                continue
            
            # ë¬¸ì¥ ë…¸ë“œë“¤ ìƒì„±
            for sentence, embedding in zip(batch, embeddings):
                try:
                    self._create_sentence_node_with_embedding(sentence, document_url, embedding)
                except Exception as e:
                    print(f"    âš ï¸ Error creating sentence node: {e}")
                    continue
    
    def _create_sentence_node_with_embedding(self, sentence: MedicalSentence, document_url: str, embedding: List[float]):
        """ì„ë² ë”©ì´ í¬í•¨ëœ ë¬¸ì¥ ë…¸ë“œ ìƒì„±"""
        
        # 1. ë¬¸ì¥ ë…¸ë“œ ìƒì„±
        sentence_query = """
        MATCH (d:Document {url: $doc_url})
        MERGE (sent:Sentence {id: $sentence_id})
        SET sent.text = $text,
            sent.section_name = $section,
            sent.embedding = $embedding,
            sent.created_at = datetime()
        MERGE (d)-[:CONTAINS_SENTENCE]->(sent)
        
        WITH sent, $section as section_title
        OPTIONAL MATCH (sec:Section) 
        WHERE sec.title = section_title
        FOREACH (s IN CASE WHEN sec IS NOT NULL THEN [sec] ELSE [] END |
            MERGE (s)-[:HAS_SENTENCE]->(sent)
        )
        
        RETURN sent
        """
        
        params = {
            "doc_url": document_url,
            "sentence_id": sentence.sentence_id,
            "text": sentence.text,
            "section": sentence.section,
            "embedding": embedding
        }
        
        self.graph.query(sentence_query, params)
        
        # 2. ì°¸ì¡°ë¬¸í—Œ ì—°ê²°
        for ref in sentence.references:
            self._create_reference_node(ref, sentence.sentence_id)
        
        # 3. ì´ë¯¸ì§€ ì—°ê²°
        for img in sentence.images:
            self._create_image_node(img, sentence.sentence_id)
        
        # 4. ì˜ë£Œ ì—”í‹°í‹° ì—°ê²°
        for entity in sentence.medical_entities:
            self._create_entity_node(entity, sentence.sentence_id)
    
    def _create_reference_node(self, reference: str, sentence_id: str):
        """ì°¸ì¡°ë¬¸í—Œ ë…¸ë“œ ìƒì„± (ID ì¶©ëŒ ë°©ì§€)"""
        ref_parts = reference.split('|')
        ref_text = ref_parts[0] if ref_parts else reference
        ref_url = ref_parts[1] if len(ref_parts) > 1 else ""
        
        query = """
        MATCH (sent:Sentence {id: $sentence_id})
        MERGE (ref:Reference {id: $ref_id})
        SET ref.text = $text,
            ref.url = $url,
            ref.created_at = datetime()
        MERGE (sent)-[:CITES]->(ref)
        RETURN ref
        """
        
        # ê³ ìœ í•œ ì°¸ì¡° ID ìƒì„±
        ref_hash = hashlib.md5(reference.encode()).hexdigest()[:8]
        ref_id = f"ref_{ref_hash}"
        
        params = {
            "sentence_id": sentence_id,
            "ref_id": ref_id,
            "text": ref_text,
            "url": ref_url
        }
        
        self.graph.query(query, params)
    
    def _create_image_node(self, image_url: str, sentence_id: str):
        """ì´ë¯¸ì§€ ë…¸ë“œ ìƒì„± (ID ì¶©ëŒ ë°©ì§€)"""
        query = """
        MATCH (sent:Sentence {id: $sentence_id})
        MERGE (img:Image {id: $image_id})
        SET img.url = $url,
            img.description = $description,
            img.created_at = datetime()
        MERGE (sent)-[:REFERENCES_IMAGE]->(img)
        RETURN img
        """
        
        # ê³ ìœ í•œ ì´ë¯¸ì§€ ID ìƒì„±
        img_hash = hashlib.md5(image_url.encode()).hexdigest()[:8]
        image_id = f"img_{img_hash}"
        
        # URLì—ì„œ ì„¤ëª… ì¶”ì¶œ ì‹œë„
        description = ""
        if "imageKey=" in image_url:
            description = image_url.split("imageKey=")[1].split("&")[0]
        elif "topicKey=" in image_url:
            description = f"Image from {image_url.split('topicKey=')[1].split('&')[0]}"
        
        params = {
            "sentence_id": sentence_id,
            "image_id": image_id,
            "url": image_url,
            "description": description
        }
        
        self.graph.query(query, params)
    
    def _create_entity_node(self, entity: str, sentence_id: str):
        """ì˜ë£Œ ì—”í‹°í‹° ë…¸ë“œ ìƒì„± (ì „ì—­ ê³ ìœ ì„± ìœ ì§€)"""
        query = """
        MATCH (sent:Sentence {id: $sentence_id})
        MERGE (entity:MedicalEntity {name: $entity_name})
        SET entity.type = $entity_type,
            entity.created_at = datetime()
        MERGE (sent)-[:MENTIONS]->(entity)
        RETURN entity
        """
        
        # ì—”í‹°í‹° íƒ€ì… ë¶„ë¥˜
        entity_type = self._classify_entity_type(entity)
        
        params = {
            "sentence_id": sentence_id,
            "entity_name": entity.lower(),  # ì „ì—­ì ìœ¼ë¡œ ê³ ìœ í•˜ê²Œ ìœ ì§€
            "entity_type": entity_type
        }
        
        self.graph.query(query, params)
    
    def _classify_entity_type(self, entity: str) -> str:
        """ì—”í‹°í‹° íƒ€ì… ë¶„ë¥˜ (í™•ì¥ëœ ë²„ì „)"""
        entity_lower = entity.lower()
        
        # ìˆ˜ìˆ /ì‹œìˆ 
        if any(term in entity_lower for term in ['pneumonectomy', 'lobectomy', 'surgery', 'resection', 'procedure', 'thoracotomy', 'intervention', 'treatment', 'therapy']):
            return 'procedure'
        # ì§ˆí™˜/ì¦ìƒ
        elif any(term in entity_lower for term in ['empyema', 'edema', 'fistula', 'syndrome', 'carcinoma', 'tumor', 'infection', 'cancer', 'disease', 'disorder', 'condition', 'infarction', 'hypertension', 'diabetes']):
            return 'condition'
        # ì¸¡ì •/ê²€ì‚¬/ë¶„ì„
        elif any(term in entity_lower for term in ['fev1', 'dlco', 'pps', 'ct', 'mri', 'radiograph', 'analysis', 'assessment', 'evaluation', 'study', 'trial', 'test', 'screening']):
            return 'measurement'
        # í•´ë¶€í•™ì  êµ¬ì¡°
        elif any(term in entity_lower for term in ['lung', 'bronchus', 'pleura', 'mediastinum', 'heart', 'ventricle', 'organ', 'tissue']):
            return 'anatomy'
        # ì•½ë¬¼/ì¹˜ë£Œ
        elif any(term in entity_lower for term in ['amiodarone', 'sotalol', 'antibiotic', 'steroid', 'drug', 'medication', 'pharmaceutical']):
            return 'medication'
        # ê²½ì œ/ë¹„ìš© ê´€ë ¨ (cost-effectiveness ë¬¸ì„œ íŠ¹í™”)
        elif any(term in entity_lower for term in ['cost', 'economic', 'financial', 'budget', 'price', 'expense', 'qaly', 'utility', 'effectiveness', 'outcome', 'benefit']):
            return 'economic'
        # ì‹œê°„/ê¸°ê°„
        elif any(term in entity_lower for term in ['year', 'month', 'day', 'time', 'period', 'duration', 'horizon']):
            return 'temporal'
        # ìˆ˜ì¹˜/í†µê³„
        elif any(char.isdigit() for char in entity) or any(term in entity_lower for term in ['percent', 'rate', 'ratio', 'probability', 'risk', 'odds']):
            return 'numeric'
        # ë°©ë²•ë¡ /ì ‘ê·¼ë²•
        elif any(term in entity_lower for term in ['method', 'approach', 'technique', 'strategy', 'model', 'framework']):
            return 'methodology'
        else:
            return 'general'
    
    def _create_vector_store(self):
        """ë²¡í„° ìŠ¤í† ì–´ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        print("ğŸ”— Creating vector store interface...")
        
        try:
            # ë²¡í„° ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            index_check = "SHOW INDEXES YIELD name WHERE name = 'sentence_vec'"
            index_exists = self.graph.query(index_check)
            
            if index_exists:
                # ê¸°ì¡´ ë…¸ë“œë“¤ì„ í™œìš©í•œ ë²¡í„° ìŠ¤í† ì–´ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
                self.vector_store = Neo4jVector(
                    embedding=self.embeddings,
                    graph=self.graph,
                    node_label="Sentence",
                    text_node_property="text",
                    embedding_node_property="embedding",
                    index_name="sentence_vec"
                )
                print("âœ… Vector store interface created with vector index")
            else:
                print("âš ï¸ Vector index not found, creating fallback vector store")
                # ë²¡í„° ì¸ë±ìŠ¤ ì—†ì´ ìƒì„±
                self.vector_store = Neo4jVector(
                    embedding=self.embeddings,
                    graph=self.graph,
                    node_label="Sentence",
                    text_node_property="text",
                    embedding_node_property="embedding"
                )
                print("âœ… Vector store interface created without vector index")
        except Exception as e:
            print(f"âš ï¸ Vector store creation warning: {e}")
            self.vector_store = None

    def get_vector_store(self):
        """ë²¡í„° ìŠ¤í† ì–´ ë°˜í™˜ (ê²€ìƒ‰ìš©)"""
        if not self.vector_store:
            try:
                self.vector_store = Neo4jVector(
                    embedding=self.embeddings,
                    graph=self.graph,
                    node_label="Sentence", 
                    text_node_property="text",
                    embedding_node_property="embedding"
                )
            except Exception as e:
                print(f"âš ï¸ Failed to create vector store: {e}")
                self.vector_store = None
        return self.vector_store
    
    def query_similar_sentences(self, query: str, k: int = 5) -> List[Dict]:
        """ìœ ì‚¬í•œ ë¬¸ì¥ ê²€ìƒ‰ (ë²¡í„° ê²€ìƒ‰)"""
        try:
            vector_store = self.get_vector_store()
            if vector_store:
                similar_docs = vector_store.similarity_search(query, k=k)
                
                results = []
                for doc in similar_docs:
                    results.append({
                        "text": doc.page_content,
                        "metadata": doc.metadata
                    })
                
                return results
            else:
                # ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ì„ ê²½ìš° í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš©
                print("âš ï¸ Vector store not available, using text-based search")
                return self._fallback_text_search(query, k)
                
        except Exception as e:
            print(f"âŒ Vector search error: {e}")
            # ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ fallback
            print("ğŸ”„ Falling back to text-based search")
            return self._fallback_text_search(query, k)
    
    def _fallback_text_search(self, query: str, k: int = 5) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰"""
        clean_query = re.sub(r'\[\d+(?:,\d+)*\]', '', query.lower()).strip()
        
        fallback_query = """
        MATCH (sent:Sentence)
        WHERE toLower(sent.text) CONTAINS $search_term
        
        OPTIONAL MATCH (sec:Section)-[:HAS_SENTENCE]->(sent)
        
        RETURN sent.text as text,
               sec.title as section,
               sent.id as sentence_id
        ORDER BY length(sent.text) ASC
        LIMIT $limit
        """
        
        try:
            results = self.graph.query(fallback_query, {"search_term": clean_query, "limit": k})
            
            formatted_results = []
            for result in results:
                formatted_results.append({
                    "text": result["text"],
                    "metadata": {
                        "section": result.get("section", "Unknown"),
                        "sentence_id": result.get("sentence_id", "")
                    }
                })
            
            return formatted_results
        except Exception as e:
            print(f"âŒ Fallback search error: {e}")
            return []
    
    def query_graph_context(self, query: str, limit: int = 10) -> List[Dict]:
        """ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ê´€ê³„ ê¸°ë°˜) - Title í¬í•¨ ê°œì„  ë²„ì „"""
        clean_query = re.sub(r'\[\d+(?:,\d+)*\]', '', query.lower()).strip()
        
        # ì¿¼ë¦¬ë¥¼ ë‹¨ì–´ë³„ë¡œ ë¶„ë¦¬í•´ì„œ ë” ìœ ì—°í•œ ê²€ìƒ‰
        query_words = [word.strip() for word in clean_query.split() if len(word.strip()) > 2]
        
        # ì—¬ëŸ¬ ê²€ìƒ‰ ì „ëµ ì‹œë„ (Title ê²€ìƒ‰ í¬í•¨)
        strategies = [
            # ì „ëµ 1: Document Title ì§ì ‘ ê²€ìƒ‰ (ìƒˆë¡œ ì¶”ê°€)
            """
            MATCH (d:Document)
            WHERE any(word IN $query_words WHERE toLower(d.title) CONTAINS word)
            
            OPTIONAL MATCH (d)-[:HAS_TITLE_SENTENCE]->(title_sent:Sentence)
            OPTIONAL MATCH (title_sent)-[:MENTIONS]->(entity:MedicalEntity)
            
            RETURN title_sent.text as sentence,
                   title_sent.id as sentence_id,
                   "DOCUMENT_TITLE" as section,
                   collect(DISTINCT entity.name) as entities,
                   [] as references,
                   [] as images,
                   "title" as result_type
            ORDER BY title_sent.text
            LIMIT $limit
            """,
            
            # ì „ëµ 2: ì—”í‹°í‹° ì´ë¦„ìœ¼ë¡œ ì§ì ‘ ê²€ìƒ‰
            """
            MATCH (entity:MedicalEntity)-[:MENTIONS]-(sent:Sentence)
            WHERE any(word IN $query_words WHERE toLower(entity.name) CONTAINS word)
            
            OPTIONAL MATCH (sent)-[:CITES]->(ref:Reference)
            OPTIONAL MATCH (sent)-[:REFERENCES_IMAGE]->(img:Image)
            OPTIONAL MATCH (sec:Section)-[:HAS_SENTENCE]->(sent)
            
            RETURN sent.text as sentence, 
                   sent.id as sentence_id,
                   COALESCE(sec.title, sent.section_name) as section,
                   collect(DISTINCT entity.name) as entities,
                   collect(DISTINCT {text: ref.text, url: ref.url}) as references,
                   collect(DISTINCT {url: img.url, description: img.description}) as images,
                   "entity" as result_type
            ORDER BY size(entities) DESC
            LIMIT $limit
            """,
            
            # ì „ëµ 3: ë¬¸ì¥ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ë³„ ê²€ìƒ‰ (Title ë¬¸ì¥ í¬í•¨)
            """
            MATCH (sent:Sentence)
            WHERE any(word IN $query_words WHERE toLower(sent.text) CONTAINS word)
            
            OPTIONAL MATCH (sent)-[:MENTIONS]->(entity:MedicalEntity)
            OPTIONAL MATCH (sent)-[:CITES]->(ref:Reference)
            OPTIONAL MATCH (sent)-[:REFERENCES_IMAGE]->(img:Image)
            OPTIONAL MATCH (sec:Section)-[:HAS_SENTENCE]->(sent)
            
            WITH sent, sec, collect(DISTINCT entity.name) as entities,
                 collect(DISTINCT {text: ref.text, url: ref.url}) as references,
                 collect(DISTINCT {url: img.url, description: img.description}) as images
            
            RETURN sent.text as sentence, 
                   sent.id as sentence_id,
                   COALESCE(sec.title, sent.section_name) as section,
                   entities,
                   references,
                   images,
                   CASE WHEN sent.is_document_title = true THEN "title" ELSE "sentence" END as result_type
            ORDER BY CASE WHEN sent.is_document_title = true THEN 0 ELSE 1 END, size(entities) DESC
            LIMIT $limit
            """,
            
            # ì „ëµ 4: ì›ë˜ ë°©ì‹ (ì „ì²´ ì¿¼ë¦¬ í…ìŠ¤íŠ¸)
            """
            MATCH (sent:Sentence)
            WHERE toLower(sent.text) CONTAINS $search_term
            
            OPTIONAL MATCH (sent)-[:MENTIONS]->(entity:MedicalEntity)
            OPTIONAL MATCH (sent)-[:CITES]->(ref:Reference)
            OPTIONAL MATCH (sent)-[:REFERENCES_IMAGE]->(img:Image)
            OPTIONAL MATCH (sec:Section)-[:HAS_SENTENCE]->(sent)
            
            WITH sent, sec, collect(DISTINCT entity.name) as entities,
                 collect(DISTINCT {text: ref.text, url: ref.url}) as references,
                 collect(DISTINCT {url: img.url, description: img.description}) as images
            
            RETURN sent.text as sentence, 
                   sent.id as sentence_id,
                   COALESCE(sec.title, sent.section_name) as section,
                   entities,
                   references,
                   images,
                   CASE WHEN sent.is_document_title = true THEN "title" ELSE "sentence" END as result_type
            ORDER BY CASE WHEN sent.is_document_title = true THEN 0 ELSE 1 END, size(entities) DESC
            LIMIT $limit
            """
        ]
        
        # ê° ì „ëµì„ ì°¨ë¡€ë¡œ ì‹œë„
        for i, strategy in enumerate(strategies, 1):
            try:
                if i <= 3:  # ë‹¨ì–´ë³„ ê²€ìƒ‰ (ì „ëµ 1, 2, 3)
                    params = {"query_words": query_words, "limit": limit}
                else:  # ì „ì²´ ì¿¼ë¦¬ ê²€ìƒ‰ (ì „ëµ 4)
                    params = {"search_term": clean_query, "limit": limit}
                
                results = self.graph.query(strategy, params)
                
                if results:
                    strategy_name = ["Title", "Entity", "Sentence", "Fallback"][i-1]
                    print(f"  âœ… ê·¸ë˜í”„ ê²€ìƒ‰ ì „ëµ {i} ({strategy_name}) ì„±ê³µ: {len(results)}ê°œ ê²°ê³¼")
                    return results
                    
            except Exception as e:
                strategy_name = ["Title", "Entity", "Sentence", "Fallback"][i-1]
                print(f"  âš ï¸ ê·¸ë˜í”„ ê²€ìƒ‰ ì „ëµ {i} ({strategy_name}) ì‹¤íŒ¨: {e}")
                continue
        
        print(f"  âŒ ëª¨ë“  ê·¸ë˜í”„ ê²€ìƒ‰ ì „ëµ ì‹¤íŒ¨")
        return []
    
    def get_document_stats(self) -> Dict:
        """ë¬¸ì„œ í†µê³„ ì¡°íšŒ"""
        stats_query = """
        MATCH (d:Document)
        OPTIONAL MATCH (d)-[:CONTAINS_SECTION]->(sec:Section)
        OPTIONAL MATCH (d)-[:CONTAINS_SENTENCE]->(sent:Sentence)
        OPTIONAL MATCH (sent)-[:MENTIONS]->(entity:MedicalEntity)
        OPTIONAL MATCH (sent)-[:CITES]->(ref:Reference)
        OPTIONAL MATCH (sent)-[:REFERENCES_IMAGE]->(img:Image)
        
        RETURN d.title as document_title,
               d.url as document_url,
               count(DISTINCT sec) as total_sections,
               count(DISTINCT sent) as total_sentences,
               count(DISTINCT entity) as total_entities,
               count(DISTINCT ref) as total_references,
               count(DISTINCT img) as total_images
        """
        
        try:
            results = self.graph.query(stats_query)
            return results[0] if results else {}
        except Exception as e:
            print(f"âŒ Stats query error: {e}")
            return {}

class MedicalRAGQueryEngine:
    """ì˜ë£Œ RAG ì¿¼ë¦¬ ì—”ì§„"""
    
    def __init__(self, graph_builder: MedicalGraphRAGBuilder):
        self.builder = graph_builder
    
    def query(self, query: str, k_vector: int = 5, k_graph: int = 10) -> Dict[str, Any]:
        """í†µí•© ì¿¼ë¦¬ ì‹¤í–‰"""
        print(f"ğŸ” Executing query: {query}")
        
        # 1. ë²¡í„° ê²€ìƒ‰
        vector_results = self.builder.query_similar_sentences(query, k=k_vector)
        
        # 2. ê·¸ë˜í”„ ê²€ìƒ‰
        graph_results = self.builder.query_graph_context(query, limit=k_graph)
        
        # 3. ê²°ê³¼ í†µí•©
        clean_query = re.sub(r'\[\d+(?:,\d+)*\]', '', query.lower()).strip()
        
        combined_results = {
            "query_info": {
                "original_query": query,
                "cleaned_query": clean_query,
                "vector_results_count": len(vector_results),
                "graph_results_count": len(graph_results)
            },
            "vector_results": vector_results,
            "graph_results": graph_results,
            "combined_sentences": self._combine_and_deduplicate(vector_results, graph_results)
        }
        
        print(f"ğŸ“Š Found {len(vector_results)} vector results, {len(graph_results)} graph results")
        
        return combined_results
    
    def _combine_and_deduplicate(self, vector_results: List[Dict], graph_results: List[Dict]) -> List[Dict]:
        """ë²¡í„°ì™€ ê·¸ë˜í”„ ê²°ê³¼ë¥¼ ê²°í•©í•˜ê³  ì¤‘ë³µ ì œê±°"""
        seen_sentences = set()
        combined = []
        
        # ë²¡í„° ê²°ê³¼ ì¶”ê°€
        for result in vector_results:
            text = result["text"]
            if text not in seen_sentences:
                seen_sentences.add(text)
                combined.append({
                    "text": text,
                    "source": "vector",
                    "metadata": result.get("metadata", {}),
                    "section": result.get("metadata", {}).get("section", "Unknown"),
                    "entities": [],
                    "references": [],
                    "images": []
                })
        
        # ê·¸ë˜í”„ ê²°ê³¼ ì¶”ê°€
        for result in graph_results:
            text = result["sentence"]
            if text not in seen_sentences:
                seen_sentences.add(text)
                combined.append({
                    "text": text,
                    "source": "graph",
                    "section": result.get("section", "Unknown"),
                    "entities": result.get("entities", []),
                    "references": [ref for ref in result.get("references", []) if ref.get("text")],
                    "images": [img for img in result.get("images", []) if img.get("url")]
                })
        
        return combined
    
    def query_by_entity(self, entity_name: str, entity_type: str = None) -> List[Dict]:
        """íŠ¹ì • ì˜ë£Œ ì—”í‹°í‹°ë¡œ ê²€ìƒ‰"""
        query = """
        MATCH (entity:MedicalEntity)-[:MENTIONS]-(sent:Sentence)
        WHERE toLower(entity.name) CONTAINS toLower($entity_name)
        """
        
        params = {"entity_name": entity_name}
        
        if entity_type:
            query += " AND entity.type = $entity_type"
            params["entity_type"] = entity_type
        
        query += """
        OPTIONAL MATCH (sec:Section)-[:HAS_SENTENCE]->(sent)
        OPTIONAL MATCH (sent)-[:CITES]->(ref:Reference)
        OPTIONAL MATCH (sent)-[:REFERENCES_IMAGE]->(img:Image)
        
        RETURN sent.text as sentence,
               sent.id as sentence_id,
               sec.title as section,
               entity.name as entity_name,
               entity.type as entity_type,
               collect(DISTINCT {text: ref.text, url: ref.url}) as references,
               collect(DISTINCT {url: img.url, description: img.description}) as images
        LIMIT 20
        """
        
        try:
            return self.builder.graph.query(query, params)
        except Exception as e:
            print(f"âŒ Entity query error: {e}")
            return []
    
    def query_by_section(self, section_title: str) -> List[Dict]:
        """íŠ¹ì • ì„¹ì…˜ì˜ ëª¨ë“  ë¬¸ì¥ ì¡°íšŒ"""
        query = """
        MATCH (sec:Section)-[:HAS_SENTENCE]->(sent:Sentence)
        WHERE toLower(sec.title) CONTAINS toLower($section_title)
        
        OPTIONAL MATCH (sent)-[:MENTIONS]->(entity:MedicalEntity)
        OPTIONAL MATCH (sent)-[:CITES]->(ref:Reference)
        OPTIONAL MATCH (sent)-[:REFERENCES_IMAGE]->(img:Image)
        
        RETURN sent.text as sentence,
               sent.id as sentence_id,
               sec.title as section,
               collect(DISTINCT entity.name) as entities,
               collect(DISTINCT {text: ref.text, url: ref.url}) as references,
               collect(DISTINCT {url: img.url, description: img.description}) as images
        ORDER BY sent.created_at
        """
        
        try:
            return self.builder.graph.query(query, {"section_title": section_title})
        except Exception as e:
            print(f"âŒ Section query error: {e}")
            return []
    
    def query_related_images(self, query: str, limit: int = 10) -> List[Dict]:
        """í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ê´€ë ¨ ì´ë¯¸ì§€ ê²€ìƒ‰"""
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ê²€ìƒ‰: {query}")
        
        clean_query = re.sub(r'\[\d+(?:,\d+)*\]', '', query.lower()).strip()
        query_words = [word.strip() for word in clean_query.split() if len(word.strip()) > 2]
        
        # ì´ë¯¸ì§€ ê²€ìƒ‰ ì „ëµë“¤
        image_strategies = [
            # ì „ëµ 1: ë¬¸ì¥ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹˜í•˜ì—¬ ì´ë¯¸ì§€ ì°¾ê¸°
            """
            MATCH (sent:Sentence)-[:REFERENCES_IMAGE]->(img:Image)
            WHERE any(word IN $query_words WHERE toLower(sent.text) CONTAINS word)
            
            OPTIONAL MATCH (sent)-[:MENTIONS]->(entity:MedicalEntity)
            OPTIONAL MATCH (sec:Section)-[:HAS_SENTENCE]->(sent)
            
            WITH sent, img, sec, collect(DISTINCT entity.name) as entities
            
            RETURN sent.text as sentence,
                   sent.id as sentence_id,
                   COALESCE(sec.title, sent.section_name) as section,
                   img.url as image_url,
                   img.description as image_description,
                   entities
            ORDER BY sent.created_at DESC
            LIMIT $limit
            """,
            
            # ì „ëµ 2: ì˜ë£Œ ì—”í‹°í‹°ë¥¼ í†µí•œ ì´ë¯¸ì§€ ê²€ìƒ‰
            """
            MATCH (entity:MedicalEntity)-[:MENTIONS]-(sent:Sentence)-[:REFERENCES_IMAGE]->(img:Image)
            WHERE any(word IN $query_words WHERE toLower(entity.name) CONTAINS word)
            
            OPTIONAL MATCH (sec:Section)-[:HAS_SENTENCE]->(sent)
            OPTIONAL MATCH (sent)-[:MENTIONS]->(all_entities:MedicalEntity)
            
            WITH sent, img, sec, entity.name as first_entity, collect(DISTINCT all_entities.name) as entities
            
            RETURN sent.text as sentence,
                   sent.id as sentence_id,
                   COALESCE(sec.title, sent.section_name) as section,
                   img.url as image_url,
                   img.description as image_description,
                   entities
            ORDER BY first_entity
            LIMIT $limit
            """,
            
            # ì „ëµ 3: ì „ì²´ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰
            """
            MATCH (sent:Sentence)-[:REFERENCES_IMAGE]->(img:Image)
            WHERE toLower(sent.text) CONTAINS $search_term
            
            OPTIONAL MATCH (sent)-[:MENTIONS]->(entity:MedicalEntity)
            OPTIONAL MATCH (sec:Section)-[:HAS_SENTENCE]->(sent)
            
            WITH sent, img, sec, collect(DISTINCT entity.name) as entities
            
            RETURN sent.text as sentence,
                   sent.id as sentence_id,
                   COALESCE(sec.title, sent.section_name) as section,
                   img.url as image_url,
                   img.description as image_description,
                   entities
            ORDER BY sent.created_at DESC
            LIMIT $limit
            """
        ]
        
        # ê° ì „ëµ ì‹œë„
        for i, strategy in enumerate(image_strategies, 1):
            try:
                if i <= 2:  # ë‹¨ì–´ë³„ ê²€ìƒ‰
                    params = {"query_words": query_words, "limit": limit}
                else:  # ì „ì²´ ì¿¼ë¦¬ ê²€ìƒ‰
                    params = {"search_term": clean_query, "limit": limit}
                
                results = self.builder.graph.query(strategy, params)
                
                if results:
                    strategy_name = ["Text Match", "Entity Match", "Full Text"][i-1]
                    print(f"  âœ… ì´ë¯¸ì§€ ê²€ìƒ‰ ì „ëµ {i} ({strategy_name}) ì„±ê³µ: {len(results)}ê°œ ì´ë¯¸ì§€")
                    return results
                    
            except Exception as e:
                strategy_name = ["Text Match", "Entity Match", "Full Text"][i-1]
                print(f"  âš ï¸ ì´ë¯¸ì§€ ê²€ìƒ‰ ì „ëµ {i} ({strategy_name}) ì‹¤íŒ¨: {e}")
                continue
        
        print(f"  âŒ ëª¨ë“  ì´ë¯¸ì§€ ê²€ìƒ‰ ì „ëµ ì‹¤íŒ¨")
        return []
    
    def query_images_by_keywords(self, keywords: List[str], limit: int = 5) -> List[Dict]:
        """íŠ¹ì • í‚¤ì›Œë“œë“¤ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰"""
        print(f"ğŸ” í‚¤ì›Œë“œ ì´ë¯¸ì§€ ê²€ìƒ‰: {keywords}")
        
        keyword_query = """
        MATCH (sent:Sentence)-[:REFERENCES_IMAGE]->(img:Image)
        WHERE any(keyword IN $keywords WHERE toLower(sent.text) CONTAINS toLower(keyword))
        
        OPTIONAL MATCH (sent)-[:MENTIONS]->(entity:MedicalEntity)
        OPTIONAL MATCH (sec:Section)-[:HAS_SENTENCE]->(sent)
        
        WITH sent, img, sec, collect(DISTINCT entity.name) as entities
        
        RETURN sent.text as sentence,
               sent.id as sentence_id,
               COALESCE(sec.title, sent.section_name) as section,
               img.url as image_url,
               img.description as image_description,
               entities
        ORDER BY sent.created_at DESC
        LIMIT $limit
        """
        
        try:
            results = self.builder.graph.query(keyword_query, {"keywords": keywords, "limit": limit})
            print(f"  âœ… í‚¤ì›Œë“œ ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê°œ ì´ë¯¸ì§€")
            return results
        except Exception as e:
            print(f"  âŒ í‚¤ì›Œë“œ ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def query_with_images(self, query: str, k_vector: int = 5, k_graph: int = 10, k_images: int = 5) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ í†µí•© ê²€ìƒ‰"""
        print(f"ğŸ” í†µí•© ê²€ìƒ‰ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€): {query}")
        
        # 1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        text_results = self.query(query, k_vector=k_vector, k_graph=k_graph)
        
        # 2. ì´ë¯¸ì§€ ê²€ìƒ‰
        image_results = self.query_related_images(query, limit=k_images)
        
        # 3. ê²°ê³¼ í†µí•©
        combined_results = text_results.copy()
        combined_results["image_results"] = image_results
        combined_results["total_images_found"] = len(image_results)
        
        # 4. ì´ë¯¸ì§€ ìš”ì•½ ì •ë³´ ì¶”ê°€
        if image_results:
            image_summary = []
            for img_result in image_results:
                image_summary.append({
                    "image_url": img_result["image_url"],
                    "image_description": img_result["image_description"],
                    "context_sentence": img_result["sentence"][:100] + "...",
                    "section": img_result["section"],
                    "entities": img_result["entities"]
                })
            combined_results["image_summary"] = image_summary
        
        print(f"ğŸ“Š í†µí•© ê²°ê³¼: í…ìŠ¤íŠ¸ {len(text_results['combined_sentences'])}ê°œ, ì´ë¯¸ì§€ {len(image_results)}ê°œ")
        
        return combined_results

# # í¸ì˜ í•¨ìˆ˜ë“¤
# def build_medical_rag_from_file(file_path: str) -> tuple:
#     """íŒŒì¼ë¡œë¶€í„° ì˜ë£Œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•"""
#     from medical_parser import parse_uptodate_file, extract_document_info, load_uptodate_document
    
#     print(f"ğŸš€ Building Medical RAG from file: {file_path}")
    
#     # 1. ë¬¸ì„œ íŒŒì‹±
#     parser = parse_uptodate_file(file_path)
    
#     # 2. ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
#     html_content = load_uptodate_document(file_path)
#     title, url = extract_document_info(html_content)
    
#     # 3. RAG ì‹œìŠ¤í…œ êµ¬ì¶•
#     builder = MedicalGraphRAGBuilder()
#     builder.build_graph_from_parser(parser, title, url)
    
#     # 4. ì¿¼ë¦¬ ì—”ì§„ ìƒì„±
#     query_engine = MedicalRAGQueryEngine(builder)
    
#     print(f"âœ… Medical RAG system ready!")
    
#     return builder, query_engine

def load_uptodate_json(file_path: str) -> Dict[str, str]:
    """UpToDate JSON íŒŒì¼ ë¡œë“œ"""
    print(f"ğŸ“‚ Loading JSON file: {file_path}")
    
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    html_content = data.get('content')
    title = data.get('title')
    url = data.get('url')
    
    print(f"âœ… Loaded: {title}")
    
    return {
        'html_content': html_content,
        'title': title, 
        'url': url
    }

def build_medical_rag_from_html(html_content: str, title: str, url: str) -> tuple:
    """HTML ë‚´ìš©ìœ¼ë¡œë¶€í„° ì˜ë£Œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•"""
    print(f"ğŸš€ Building Medical RAG from HTML: {title}")
    
    # RAG ì‹œìŠ¤í…œ êµ¬ì¶•
    builder = MedicalGraphRAGBuilder()
    builder.build_graph_from_html(html_content, title, url)
    
    # ì¿¼ë¦¬ ì—”ì§„ ìƒì„±
    query_engine = MedicalRAGQueryEngine(builder)
    
    print(f"âœ… Medical RAG system ready!")
    
    return builder, query_engine

def build_medical_rag_from_json(json_file_path: str) -> tuple:
    """JSON íŒŒì¼ë¡œë¶€í„° ì˜ë£Œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•"""
    # JSON íŒŒì¼ ë¡œë“œ
    data = load_uptodate_json(json_file_path)
    
    # RAG ì‹œìŠ¤í…œ êµ¬ì¶•
    return build_medical_rag_from_html(
        data['html_content'], 
        data['title'], 
        data['url']
    )

def build_medical_rag_from_directory(directory_path: str = "data/uptodate/general-surgery", max_files: int = None) -> tuple:
    """ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  JSON íŒŒì¼ë“¤ë¡œë¶€í„° ì˜ë£Œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•"""
    import glob
    import os
    import sys
    from pathlib import Path
    from tqdm import tqdm
    from contextlib import redirect_stdout, redirect_stderr
    from io import StringIO
    
    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    log_dir = Path(directory_path).parent / "logs"
    log_dir.mkdir(exist_ok=True)
    log_file = log_dir / "medical_rag_build.log"
    
    print(f"ğŸš€ Building Medical RAG from directory: {directory_path}")
    print(f"ğŸ“ Build logs will be saved to: {log_file}")
    
    # ë””ë ‰í† ë¦¬ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
    if not os.path.isabs(directory_path):
        directory_path = os.path.join(os.getcwd(), directory_path)
    
    # JSON íŒŒì¼ë“¤ ì°¾ê¸°
    json_pattern = os.path.join(directory_path, "*.json")
    all_json_files = glob.glob(json_pattern)
    
    if not all_json_files:
        raise FileNotFoundError(f"No JSON files found in directory: {directory_path}")
    
    # íŒŒì¼ ìˆ˜ ì œí•œ ì ìš©
    if max_files and max_files > 0:
        json_files = all_json_files[:max_files]
        print(f"ğŸ“‚ Found {len(all_json_files)} JSON files, processing first {len(json_files)} files")
    else:
        json_files = all_json_files
        print(f"ğŸ“‚ Found {len(json_files)} JSON files to process")
        
        # ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ì‹œ í™•ì¸ ìš”ì²­
        if len(json_files) > 100:
            print(f"âš ï¸ Processing {len(json_files)} files will take a long time!")
            print(f"ğŸ’¡ Consider using --max-files option to limit the number of files")
    
    # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
    successful_files = []
    failed_files = []
    
    try:
        # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ tqdm
        with tqdm(total=len(json_files), desc="ğŸ¥ Processing medical documents", 
                  bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]") as pbar:
            
            for i, json_file in enumerate(json_files):
                file_name = Path(json_file).name
                pbar.set_description(f"ğŸ¥ Processing: {file_name[:50]}...")
                
                # ë¡œê·¸ë¥¼ íŒŒì¼ê³¼ ë²„í¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
                with open(log_file, 'a', encoding='utf-8') as f:
                    with redirect_stdout(f), redirect_stderr(f):
                        f.write(f"\n{'='*80}\n")
                        f.write(f"Processing file {i+1}/{len(json_files)}: {file_name}\n")
                        f.write(f"{'='*80}\n")
                        
                        try:
                            if i == 0:
                                # ì²« ë²ˆì§¸ íŒŒì¼ë¡œ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
                                f.write(f"ğŸ—ï¸ Initializing RAG system with: {file_name}\n")
                                builder, query_engine = build_medical_rag_from_json(json_file)
                            else:
                                # JSON íŒŒì¼ ë¡œë“œ ë° ì¶”ê°€
                                data = load_uptodate_json(json_file)
                                builder.build_graph_from_html(
                                    data['html_content'], 
                                    data['title'], 
                                    data['url']
                                )
                            
                            successful_files.append(json_file)
                            f.write(f"âœ… Successfully processed: {file_name}\n")
                            
                        except Exception as e:
                            error_msg = f"âŒ Failed to process {file_name}: {e}"
                            f.write(f"{error_msg}\n")
                            failed_files.append((json_file, str(e)))
                
                pbar.update(1)
                pbar.set_postfix(success=len(successful_files), failed=len(failed_files))
    
    finally:
        # ë¹Œë“œ ì™„ë£Œ í›„ ëª¨ë“  ì¶œë ¥ ë³µì›
        print(f"\nğŸ‰ Build completed!")
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        print(f"\nğŸ“Š Processing Summary:")
        print(f"  âœ… Successfully processed: {len(successful_files)} files")
        print(f"  âŒ Failed: {len(failed_files)} files")
        
        if failed_files:
            print(f"\nâš ï¸ Failed files:")
            for failed_file, error in failed_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"    {Path(failed_file).name}: {error}")
            if len(failed_files) > 5:
                print(f"    ... and {len(failed_files) - 5} more (see log file)")
        
        # ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
        if successful_files:
            total_stats = builder.get_document_stats()
            print(f"\nğŸ“ˆ Combined Database Statistics:")
            for key, value in total_stats.items():
                print(f"  {key}: {value}")
        
        print(f"\nğŸ“ Detailed logs saved to: {log_file}")
        print(f"âœ… Multi-document RAG system ready with {len(successful_files)} documents!")
    
    if not successful_files:
        raise RuntimeError("No files were successfully processed")
    
    return builder, query_engine

def get_directory_statistics(directory_path: str = "data/uptodate/general-surgery") -> Dict:
    """ë””ë ‰í† ë¦¬ì˜ JSON íŒŒì¼ë“¤ì— ëŒ€í•œ í†µê³„ ì¡°íšŒ"""
    import glob
    import os
    from pathlib import Path
    
    # ë””ë ‰í† ë¦¬ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
    if not os.path.isabs(directory_path):
        directory_path = os.path.join(os.getcwd(), directory_path)
    
    # JSON íŒŒì¼ë“¤ ì°¾ê¸°
    json_pattern = os.path.join(directory_path, "*.json")
    json_files = glob.glob(json_pattern)
    
    stats = {
        "directory_path": directory_path,
        "total_json_files": len(json_files),
        "file_sizes_mb": [],
        "sample_files": []
    }
    
    for json_file in json_files[:10]:  # ì²˜ìŒ 10ê°œ íŒŒì¼ë§Œ ìƒ˜í”Œë§
        try:
            file_size = os.path.getsize(json_file) / (1024 * 1024)  # MB ë‹¨ìœ„
            stats["file_sizes_mb"].append(round(file_size, 2))
            
            # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ
            file_name = Path(json_file).stem
            stats["sample_files"].append(file_name)
            
        except Exception as e:
            continue
    
    if stats["file_sizes_mb"]:
        stats["avg_file_size_mb"] = round(sum(stats["file_sizes_mb"]) / len(stats["file_sizes_mb"]), 2)
        stats["total_size_mb"] = round(sum(stats["file_sizes_mb"]), 2)
    
    return stats

if __name__ == "__main__":
    import argparse
    import sys
    
    parser = argparse.ArgumentParser(description='ì˜ë£Œ ë¬¸ì„œìš© Graph RAG ì‹œìŠ¤í…œ')
    
    # ë””ë ‰í† ë¦¬ ëª¨ë“œì™€ ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ ì§€ì›
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--directory', '-d', 
                      default="data/uptodate/general-surgery",
                      help='UpToDate JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: data/uptodate/general-surgery)')
    group.add_argument('--json-file', '-f', 
                      help='ë‹¨ì¼ UpToDate JSON íŒŒì¼ ê²½ë¡œ')
    
    parser.add_argument('--test-queries', action='store_true', help='í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰')
    parser.add_argument('--stats-only', action='store_true', help='ë””ë ‰í† ë¦¬ í†µê³„ë§Œ ì¡°íšŒ')
    parser.add_argument('--max-files', type=int, help='ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ ìˆ˜ (ë””ë ‰í† ë¦¬ ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©)')
    
    args = parser.parse_args()
    
    try:
        # í†µê³„ë§Œ ì¡°íšŒí•˜ëŠ” ê²½ìš°
        if args.stats_only:
            directory_path = args.directory
            print(f"ğŸ“Š Directory Statistics: {directory_path}")
            print("=" * 50)
            
            stats = get_directory_statistics(directory_path)
            print(f"ğŸ“‚ Total JSON files: {stats['total_json_files']}")
            
            if stats['file_sizes_mb']:
                print(f"ğŸ“ Average file size: {stats['avg_file_size_mb']} MB")
                print(f"ğŸ’½ Total size: {stats['total_size_mb']} MB")
                
                print(f"\nğŸ“„ Sample files:")
                for i, file_name in enumerate(stats['sample_files'][:5], 1):
                    print(f"  {i}. {file_name}")
                
                if len(stats['sample_files']) < stats['total_json_files']:
                    remaining = stats['total_json_files'] - len(stats['sample_files'])
                    print(f"  ... ë° {remaining}ê°œ íŒŒì¼ ë”")
            
            sys.exit(0)
        
        # RAG ì‹œìŠ¤í…œ êµ¬ì¶•
        if args.json_file:
            # ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ
            print(f"ğŸ“„ Single file mode: {args.json_file}")
            builder, query_engine = build_medical_rag_from_json(args.json_file)
        else:
            # ë””ë ‰í† ë¦¬ ëª¨ë“œ (ê¸°ë³¸ê°’)
            print(f"ğŸ“‚ Directory mode: {args.directory}")
            if args.max_files:
                print(f"ğŸ”¢ Limiting to {args.max_files} files")
            builder, query_engine = build_medical_rag_from_directory(args.directory, max_files=args.max_files)
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰ (ì˜µì…˜)
        if args.test_queries:
            # ì˜ë£Œ ë¬¸ì„œì— ë§ëŠ” ì¿¼ë¦¬ë“¤
            test_queries = [
                "cancer treatment chemotherapy",
                "surgical complications management", 
                "metastases diagnosis imaging",
                "patient prognosis survival rate",
                "clinical trial evidence",
                "tumor resection procedure"
            ]
            
            print(f"\nğŸ§ª Testing queries...")
            for query in test_queries:
                print(f"\nğŸ” Query: {query}")
                results = query_engine.query(query, k_vector=3, k_graph=3)
                
                vector_count = results['query_info']['vector_results_count']
                graph_count = results['query_info']['graph_results_count']
                print(f"ğŸ“Š Results: {vector_count} vector + {graph_count} graph")
                
                # ìƒìœ„ ê²°ê³¼ ì¶œë ¥
                if results['combined_sentences']:
                    top_result = results['combined_sentences'][0]
                    print(f"ğŸ¯ Top result: {top_result['text'][:100]}...")
                    print(f"   Section: {top_result['section']}")
                    print(f"   Source: {top_result['source']}")
                    if top_result['entities']:
                        print(f"   Entities: {', '.join(top_result['entities'][:3])}")
        
        print(f"\nâœ… RAG ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
        
        if args.json_file:
            print(f"  # ë‹¨ì¼ íŒŒì¼")
            print(f"  from medical_graph_rag import build_medical_rag_from_json")
            print(f"  builder, query_engine = build_medical_rag_from_json('{args.json_file}')")
        else:
            print(f"  # ë””ë ‰í† ë¦¬ ì „ì²´")
            print(f"  from medical_graph_rag import build_medical_rag_from_directory")
            print(f"  builder, query_engine = build_medical_rag_from_directory('{args.directory}')")
        
        print(f"  results = query_engine.query('your medical question')")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()