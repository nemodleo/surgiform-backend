"""
ì˜ë£Œ ë¬¸ì„œìš© Graph RAG ì‹œìŠ¤í…œ - í¸ì˜ í•¨ìˆ˜ë“¤ê³¼ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
Graph RAG êµ¬ì¶•ê³¼ ê²€ìƒ‰ì„ ìœ„í•œ ê³ ìˆ˜ì¤€ ì¸í„°í˜ì´ìŠ¤
"""

import json
import os
import sys
import glob
from pathlib import Path
from typing import Dict

from surgiform.core.ingest.uptodate.medical_graph_builder import MedicalGraphRAGBuilder
from surgiform.core.ingest.uptodate.medical_rag_engine import MedicalRAGQueryEngine


def load_uptodate_json(file_path: str) -> Dict[str, str]:
    """UpToDate JSON íŒŒì¼ ë¡œë“œ"""
    print(f"ğŸ“‚ Loading JSON file: {file_path}")
    
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    html_content = data.get('content')
    title = data.get('title')
    url = data.get('url')
    
    print(f"âœ… Loaded: {title}")
    
    return {
        'html_content': html_content,
        'title': title, 
        'url': url
    }

def build_medical_rag_from_html(html_content: str, title: str, url: str) -> tuple:
    """HTML ë‚´ìš©ìœ¼ë¡œë¶€í„° ì˜ë£Œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•"""
    print(f"ğŸš€ Building Medical RAG from HTML: {title}")
    
    # RAG ì‹œìŠ¤í…œ êµ¬ì¶•
    builder = MedicalGraphRAGBuilder()
    builder.build_graph_from_html(html_content, title, url)
    
    # ì¿¼ë¦¬ ì—”ì§„ ìƒì„±
    query_engine = MedicalRAGQueryEngine(builder)
    
    print(f"âœ… Medical RAG system ready!")
    
    return builder, query_engine

def build_medical_rag_from_json(json_file_path: str) -> tuple:
    """JSON íŒŒì¼ë¡œë¶€í„° ì˜ë£Œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•"""
    # JSON íŒŒì¼ ë¡œë“œ
    data = load_uptodate_json(json_file_path)
    
    # RAG ì‹œìŠ¤í…œ êµ¬ì¶•
    return build_medical_rag_from_html(
        data['html_content'], 
        data['title'], 
        data['url']
    )

def build_medical_rag_from_directory(directory_path: str = "data/uptodate/general-surgery", max_files: int = None) -> tuple:
    """ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  JSON íŒŒì¼ë“¤ë¡œë¶€í„° ì˜ë£Œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•"""
    import glob
    import os
    import sys
    from pathlib import Path
    from tqdm import tqdm
    from contextlib import redirect_stdout, redirect_stderr
    from io import StringIO
    
    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    log_dir = Path(directory_path).parent / "logs"
    log_dir.mkdir(exist_ok=True)
    log_file = log_dir / "medical_rag_build.log"
    
    print(f"ğŸš€ Building Medical RAG from directory: {directory_path}")
    print(f"ğŸ“ Build logs will be saved to: {log_file}")
    
    # ë””ë ‰í† ë¦¬ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
    if not os.path.isabs(directory_path):
        directory_path = os.path.join(os.getcwd(), directory_path)
    
    # JSON íŒŒì¼ë“¤ ì°¾ê¸°
    json_pattern = os.path.join(directory_path, "*.json")
    all_json_files = glob.glob(json_pattern)
    
    if not all_json_files:
        raise FileNotFoundError(f"No JSON files found in directory: {directory_path}")
    
    # íŒŒì¼ ìˆ˜ ì œí•œ ì ìš©
    if max_files and max_files > 0:
        json_files = all_json_files[:max_files]
        print(f"ğŸ“‚ Found {len(all_json_files)} JSON files, processing first {len(json_files)} files")
    else:
        json_files = all_json_files
        print(f"ğŸ“‚ Found {len(json_files)} JSON files to process")
        
        # ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ì‹œ í™•ì¸ ìš”ì²­
        if len(json_files) > 100:
            print(f"âš ï¸ Processing {len(json_files)} files will take a long time!")
            print(f"ğŸ’¡ Consider using --max-files option to limit the number of files")
    
    # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„ ë° ë³€ìˆ˜ ì´ˆê¸°í™”
    successful_files = []
    failed_files = []
    builder = None
    query_engine = None
    rag_initialized = False
    
    try:
        # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ tqdm
        with tqdm(total=len(json_files), desc="ğŸ¥ Processing medical documents", 
                  bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]") as pbar:
            
            for i, json_file in enumerate(json_files):
                file_name = Path(json_file).name
                pbar.set_description(f"ğŸ¥ Processing: {file_name[:50]}...")
                
                # ë¡œê·¸ë¥¼ íŒŒì¼ê³¼ ë²„í¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
                with open(log_file, 'a', encoding='utf-8') as f:
                    with redirect_stdout(f), redirect_stderr(f):
                        f.write(f"\n{'='*80}\n")
                        f.write(f"Processing file {i+1}/{len(json_files)}: {file_name}\n")
                        f.write(f"{'='*80}\n")
                        
                        try:
                            if not rag_initialized:
                                # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì²« ì„±ê³µí•œ íŒŒì¼ë¡œ)
                                f.write(f"ğŸ—ï¸ Initializing RAG system with: {file_name}\n")
                                builder, query_engine = build_medical_rag_from_json(json_file)
                                rag_initialized = True
                            else:
                                # JSON íŒŒì¼ ë¡œë“œ ë° ì¶”ê°€
                                data = load_uptodate_json(json_file)
                                builder.build_graph_from_html(
                                    data['html_content'], 
                                    data['title'], 
                                    data['url']
                                )
                            
                            successful_files.append(json_file)
                            f.write(f"âœ… Successfully processed: {file_name}\n")
                            
                        except Exception as e:
                            error_msg = f"âŒ Failed to process {file_name}: {e}"
                            f.write(f"{error_msg}\n")
                            failed_files.append((json_file, str(e)))
                            # RAGê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ ì‹¤íŒ¨í•œ ê²½ìš° ë‹¤ìŒ íŒŒì¼ë¡œ ê³„ì† ì§„í–‰
                
                pbar.update(1)
                pbar.set_postfix(success=len(successful_files), failed=len(failed_files))
    
    finally:
        # ë¹Œë“œ ì™„ë£Œ í›„ ëª¨ë“  ì¶œë ¥ ë³µì›
        print(f"\nğŸ‰ Build completed!")
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        print(f"\nğŸ“Š Processing Summary:")
        print(f"  âœ… Successfully processed: {len(successful_files)} files")
        print(f"  âŒ Failed: {len(failed_files)} files")
        
        if failed_files:
            print(f"\nâš ï¸ Failed files:")
            for failed_file, error in failed_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"    {Path(failed_file).name}: {error}")
            if len(failed_files) > 5:
                print(f"    ... and {len(failed_files) - 5} more (see log file)")
        
        # ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
        if successful_files and builder is not None:
            total_stats = builder.get_document_stats()
            print(f"\nğŸ“ˆ Combined Database Statistics:")
            for key, value in total_stats.items():
                print(f"  {key}: {value}")
        
        print(f"\nğŸ“ Detailed logs saved to: {log_file}")
        
        if builder is not None:
            print(f"âœ… Multi-document RAG system ready with {len(successful_files)} documents!")
        else:
            print(f"âŒ RAG system could not be initialized - all files failed to process")
    
    if not successful_files or builder is None:
        raise RuntimeError("No files were successfully processed or RAG system could not be initialized")
    
    return builder, query_engine

def get_directory_statistics(directory_path: str = "data/uptodate/general-surgery") -> Dict:
    """ë””ë ‰í† ë¦¬ì˜ JSON íŒŒì¼ë“¤ì— ëŒ€í•œ í†µê³„ ì¡°íšŒ"""
    import glob
    import os
    from pathlib import Path
    
    # ë””ë ‰í† ë¦¬ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
    if not os.path.isabs(directory_path):
        directory_path = os.path.join(os.getcwd(), directory_path)
    
    # JSON íŒŒì¼ë“¤ ì°¾ê¸°
    json_pattern = os.path.join(directory_path, "*.json")
    json_files = glob.glob(json_pattern)
    
    stats = {
        "directory_path": directory_path,
        "total_json_files": len(json_files),
        "file_sizes_mb": [],
        "sample_files": []
    }
    
    for json_file in json_files[:10]:  # ì²˜ìŒ 10ê°œ íŒŒì¼ë§Œ ìƒ˜í”Œë§
        try:
            file_size = os.path.getsize(json_file) / (1024 * 1024)  # MB ë‹¨ìœ„
            stats["file_sizes_mb"].append(round(file_size, 2))
            
            # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œ
            file_name = Path(json_file).stem
            stats["sample_files"].append(file_name)
            
        except Exception as e:
            continue
    
    if stats["file_sizes_mb"]:
        stats["avg_file_size_mb"] = round(sum(stats["file_sizes_mb"]) / len(stats["file_sizes_mb"]), 2)
        stats["total_size_mb"] = round(sum(stats["file_sizes_mb"]), 2)
    
    return stats

if __name__ == "__main__":
    import argparse
    import sys
    
    parser = argparse.ArgumentParser(description='ì˜ë£Œ ë¬¸ì„œìš© Graph RAG ì‹œìŠ¤í…œ')
    
    # ë””ë ‰í† ë¦¬ ëª¨ë“œì™€ ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ ì§€ì›
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--directory', '-d', 
                      default="data/uptodate/general-surgery",
                      help='UpToDate JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: data/uptodate/general-surgery)')
    group.add_argument('--json-file', '-f', 
                      help='ë‹¨ì¼ UpToDate JSON íŒŒì¼ ê²½ë¡œ')
    
    parser.add_argument('--test-queries', action='store_true', help='í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰')
    parser.add_argument('--stats-only', action='store_true', help='ë””ë ‰í† ë¦¬ í†µê³„ë§Œ ì¡°íšŒ')
    parser.add_argument('--max-files', type=int, help='ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ ìˆ˜ (ë””ë ‰í† ë¦¬ ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©)')
    
    args = parser.parse_args()
    
    try:
        # í†µê³„ë§Œ ì¡°íšŒí•˜ëŠ” ê²½ìš°
        if args.stats_only:
            directory_path = args.directory
            print(f"ğŸ“Š Directory Statistics: {directory_path}")
            print("=" * 50)
            
            stats = get_directory_statistics(directory_path)
            print(f"ğŸ“‚ Total JSON files: {stats['total_json_files']}")
            
            if stats['file_sizes_mb']:
                print(f"ğŸ“ Average file size: {stats['avg_file_size_mb']} MB")
                print(f"ğŸ’½ Total size: {stats['total_size_mb']} MB")
                
                print(f"\nğŸ“„ Sample files:")
                for i, file_name in enumerate(stats['sample_files'][:5], 1):
                    print(f"  {i}. {file_name}")
                
                if len(stats['sample_files']) < stats['total_json_files']:
                    remaining = stats['total_json_files'] - len(stats['sample_files'])
                    print(f"  ... ë° {remaining}ê°œ íŒŒì¼ ë”")
            
            sys.exit(0)
        
        # RAG ì‹œìŠ¤í…œ êµ¬ì¶•
        if args.json_file:
            # ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ
            print(f"ğŸ“„ Single file mode: {args.json_file}")
            builder, query_engine = build_medical_rag_from_json(args.json_file)
        else:
            # ë””ë ‰í† ë¦¬ ëª¨ë“œ (ê¸°ë³¸ê°’)
            print(f"ğŸ“‚ Directory mode: {args.directory}")
            if args.max_files:
                print(f"ğŸ”¢ Limiting to {args.max_files} files")
            builder, query_engine = build_medical_rag_from_directory(args.directory, max_files=args.max_files)
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰ (ì˜µì…˜)
        if args.test_queries:
            # ì˜ë£Œ ë¬¸ì„œì— ë§ëŠ” ì¿¼ë¦¬ë“¤
            test_queries = [
                "cancer treatment chemotherapy",
                "surgical complications management", 
                "metastases diagnosis imaging",
                "patient prognosis survival rate",
                "clinical trial evidence",
                "tumor resection procedure"
            ]
            
            print(f"\nğŸ§ª Testing queries...")
            for query in test_queries:
                print(f"\nğŸ” Query: {query}")
                        
                # ì™„ì „ í†µí•© ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                results = query_engine.query_all_elements(query, k_vector=2, k_graph=2, k_images=2, k_tables=2)
                
                vector_count = results['query_info']['vector_results_count']
                graph_count = results['query_info']['graph_results_count']
                images_count = results['total_images_found']
                tables_count = results['total_tables_found']
                print(f"ğŸ“Š Results: {vector_count} vector + {graph_count} graph + {images_count} images + {tables_count} tables")
                
                # ìƒìœ„ ê²°ê³¼ ì¶œë ¥
                if results['combined_sentences']:
                    top_result = results['combined_sentences'][0]
                    print(f"ğŸ¯ Top result: {top_result['text'][:100]}...")
                    print(f"   Section: {top_result['section']}")
                    print(f"   Source: {top_result['source']}")
                    if top_result['entities']:
                        print(f"   Entities: {', '.join(top_result['entities'][:3])}")
                    if top_result['references']:
                        print(f"   References: {len(top_result['references'])} found")
                    if top_result['images']:
                        print(f"   Images: {len(top_result['images'])} found")
                    if top_result['tables']:
                        print(f"   Tables: {len(top_result['tables'])} found")
        
        print(f"\nâœ… RAG ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
        
        if args.json_file:
            print(f"  # ë‹¨ì¼ íŒŒì¼")
            print(f"  from medical_graph_rag import build_medical_rag_from_json")
            print(f"  builder, query_engine = build_medical_rag_from_json('{args.json_file}')")
        else:
            print(f"  # ë””ë ‰í† ë¦¬ ì „ì²´")
            print(f"  from medical_graph_rag import build_medical_rag_from_directory")
            print(f"  builder, query_engine = build_medical_rag_from_directory('{args.directory}')")
            print(f"  results = query_engine.query('your medical question')")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
