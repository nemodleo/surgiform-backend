"""
UpToDate ì˜ë£Œ ë¬¸ì„œ íŒŒì„œ
HTML ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ì—¬ ì„¹ì…˜, ë¬¸ì¥, ì°¸ì¡°ë¬¸í—Œ, ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì¶”ì¶œ
"""

import re
import hashlib
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from bs4 import BeautifulSoup

@dataclass
class MedicalSentence:
    """ì˜ë£Œ ë¬¸ì¥ ì •ë³´"""
    text: str
    section: str
    references: List[str]
    images: List[str]
    medical_entities: List[str]
    sentence_id: str

@dataclass
class MedicalSection:
    """ì˜ë£Œ ì„¹ì…˜ ì •ë³´"""
    title: str
    content: str
    level: int  # h1=1, h2=2, etc.
    section_id: str

class MedicalDocumentParser:
    """UpToDate ì˜ë£Œ ë¬¸ì„œ íŒŒì„œ"""
    
    def __init__(self, document_url: str = ""):
        self.soup = None
        self.sections = []
        self.sentences = []
        self.doc_hash = hashlib.md5(document_url.encode()).hexdigest()[:8] if document_url else "default"
        
    def parse_html(self, html_content: str) -> None:
        """HTML ë¬¸ì„œ íŒŒì‹± - ë©”ì¸ ì§„ì…ì """
        print(f"ğŸ”„ Starting HTML parsing...")
        
        self.soup = BeautifulSoup(html_content, 'html.parser')
        self._extract_sections()
        self._extract_sentences()
        
        print(f"âœ… Parsing complete: {len(self.sections)} sections, {len(self.sentences)} sentences")
    
    def _extract_sections(self) -> None:
        """ì„¹ì…˜ ì¶”ì¶œ (UpToDate ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ)"""
        print("ğŸ“‘ Extracting sections...")
        sections = []
        
        # UpToDateì˜ ì‹¤ì œ ì œëª© êµ¬ì¡°: <p class="headingAnchor css_h1|css_h2">
        headings = self.soup.find_all(lambda tag: 
            tag.name == 'p' and 
            tag.has_attr('class') and 
            'headingAnchor' in ' '.join(tag['class'])
        )
        
        print(f"ğŸ“‹ Found {len(headings)} headings")
        
        for i, heading in enumerate(headings):
            # ì œëª© í…ìŠ¤íŠ¸ ì¶”ì¶œ - span.h1 ë˜ëŠ” span.h2ì—ì„œ
            title_span = heading.find('span', class_=lambda x: x and ('h1' in x or 'h2' in x))
            title = title_span.get_text(strip=True) if title_span else heading.get_text(strip=True)
            
            # ë¹ˆ ì œëª© ìŠ¤í‚µ
            if not title or len(title.strip()) < 2:
                continue
            
            # ì„¹ì…˜ ë ˆë²¨ ê²°ì • (css_h1=1, css_h2=2)
            level = 1 if 'css_h1' in ' '.join(heading['class']) else 2
            
            # ì„¹ì…˜ ID - ë¬¸ì„œë³„ ê³ ìœ í•˜ê²Œ ìƒì„±
            raw_id = heading.get('id', f'section_{i}')
            section_id = f"{self.doc_hash}_{raw_id}"
            
            # ë‹¤ìŒ ì œëª©ê¹Œì§€ì˜ ë‚´ìš© ì¶”ì¶œ
            content = self._extract_section_content(heading, headings, i)
            
            section = MedicalSection(
                title=title,
                content=content,
                level=level,
                section_id=section_id
            )
            sections.append(section)
            
            print(f"  ğŸ“„ Section {i+1}: {title[:50]}{'...' if len(title) > 50 else ''}")
        
        self.sections = sections
        print(f"âœ… Extracted {len(sections)} sections")
    
    def _extract_section_content(self, heading, all_headings: List, current_index: int) -> str:
        """ì„¹ì…˜ ë‚´ìš© ì¶”ì¶œ"""
        content_elements = []
        current = heading.find_next_sibling()
        next_heading = all_headings[current_index + 1] if current_index + 1 < len(all_headings) else None
        
        while current and current != next_heading:
            if current.name in ['p', 'ul', 'ol', 'div'] and current.get_text(strip=True):
                # ì°¸ì¡° ë²ˆí˜¸ ì œê±°í•´ì„œ ê¹”ë”í•œ ë‚´ìš©ë§Œ 
                text = current.get_text(strip=True)
                clean_text = re.sub(r'\[\d+(?:,\d+)*\]', '', text)
                if clean_text.strip() and len(clean_text.strip()) > 10:
                    content_elements.append(clean_text.strip())
            current = current.find_next_sibling()
        
        return '\n'.join(content_elements)
    
    def _extract_sentences(self) -> None:
        """ë¬¸ì¥ë³„ë¡œ ì¶”ì¶œí•˜ê³  ì°¸ì¡°ë¬¸í—Œ, ì´ë¯¸ì§€ ì •ë³´ í¬í•¨"""
        print("ğŸ“ Extracting sentences...")
        sentences = []
        sentence_id = 0
        
        # ë‹¤ì–‘í•œ ë¬¸ë‹¨ í´ë˜ìŠ¤ ì°¾ê¸°
        paragraph_classes = ['css_h1', 'css_h2', 'bulletIndent1', 'bulletIndent2', 'inlineGraphicParagraph']
        
        paragraphs = []
        for cls in paragraph_classes:
            found_paras = self.soup.find_all('p', class_=lambda x: x and cls in x)
            paragraphs.extend(found_paras)
        
        # ì¤‘ë³µ ì œê±° (ê°™ì€ ë¬¸ë‹¨ì´ ì—¬ëŸ¬ í´ë˜ìŠ¤ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŒ)
        paragraphs = list(set(paragraphs))
        
        print(f"ğŸ“„ Found {len(paragraphs)} paragraphs")
        
        for para_idx, para in enumerate(paragraphs):
            if para_idx % 50 == 0:
                print(f"  ğŸ”„ Processing paragraph {para_idx + 1}/{len(paragraphs)}")
            
            # í˜„ì¬ ì„¹ì…˜ ì°¾ê¸°
            current_section = self._find_section_for_element(para)
            
            # ë¬¸ë‹¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            para_text = para.get_text(strip=True)
            if not para_text or len(para_text) < 20:
                continue
            
            # ë¬¸ì¥ ë¶„ë¦¬
            sentences_in_para = self._split_sentences(para_text)
            
            for sentence_text in sentences_in_para:
                if len(sentence_text.strip()) < 15:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                    continue
                
                # ì°¸ì¡°ë¬¸í—Œ ì¶”ì¶œ
                references = self._extract_references_from_element(para)
                
                # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
                images = self._extract_images_from_element(para)
                
                # ì˜ë£Œ ì—”í‹°í‹° ì¶”ì¶œ
                medical_entities = self._extract_medical_entities(sentence_text)
                
                # ë¬¸ì¥ ID - ë¬¸ì„œë³„ ê³ ìœ í•˜ê²Œ ìƒì„±
                unique_sentence_id = f"{self.doc_hash}_sent_{sentence_id}"
                
                sentence = MedicalSentence(
                    text=sentence_text.strip(),
                    section=current_section,
                    references=references,
                    images=images,
                    medical_entities=medical_entities,
                    sentence_id=unique_sentence_id
                )
                sentences.append(sentence)
                sentence_id += 1
        
        self.sentences = sentences
        print(f"âœ… Extracted {len(sentences)} sentences")
    
    def _find_section_for_element(self, element) -> str:
        """ìš”ì†Œê°€ ì†í•œ ì„¹ì…˜ ì°¾ê¸° (UpToDate êµ¬ì¡°ì— ë§ê²Œ)"""
        # ì´ì „ ì œëª© ì°¾ê¸° - p.headingAnchor í˜•íƒœ
        heading = element.find_previous('p', class_=lambda x: x and 'headingAnchor' in x)
        if heading:
            title_span = heading.find('span', class_=lambda x: x and ('h1' in x or 'h2' in x))
            if title_span:
                return title_span.get_text(strip=True)
            else:
                # spanì´ ì—†ëŠ” ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                heading_text = heading.get_text(strip=True)
                # ì œëª©ì—ì„œ ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±°
                clean_title = re.sub(r'^[^\w]*|[^\w]*$', '', heading_text)
                return clean_title if clean_title else "Unknown Section"
        return "Unknown Section"
    
    def _split_sentences(self, text: str) -> List[str]:
        """ë¬¸ì¥ ë¶„ë¦¬ (ê°œì„ ëœ ë²„ì „)"""
        # ì˜ë£Œ ë¬¸ì„œ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë¬¸ì¥ ë¶„ë¦¬
        # ì•½ì–´ ë’¤ì˜ ë§ˆì¹¨í‘œëŠ” ë¬¸ì¥ ëì´ ì•„ë‹˜
        abbreviations = ['Dr', 'Mr', 'Ms', 'vs', 'eg', 'ie', 'etc', 'al', 'FEV1', 'DLCO', 'PPS']
        
        # ì•½ì–´ ë³´í˜¸
        protected_text = text
        for abbr in abbreviations:
            protected_text = protected_text.replace(f'{abbr}.', f'{abbr}<!PERIOD!>')
        
        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', protected_text)
        
        # ì•½ì–´ ë³µì› ë° ì •ë¦¬
        result = []
        for sentence in sentences:
            restored = sentence.replace('<!PERIOD!>', '.')
            cleaned = restored.strip()
            if cleaned and len(cleaned) > 10:
                result.append(cleaned)
        
        return result
    
    def _extract_references_from_element(self, element) -> List[str]:
        """ìš”ì†Œì—ì„œ ì°¸ì¡°ë¬¸í—Œ ì¶”ì¶œ"""
        references = []
        
        # ì°¸ì¡° ë§í¬ ì°¾ê¸° (ë‹¤ì–‘í•œ íŒ¨í„´)
        ref_selectors = [
            'a.abstract_t',  # ì¼ë°˜ ì°¸ì¡°
            'a[href*="/abstract/"]',  # ì¶”ìƒ ë§í¬
            'a[class*="reference"]'  # ì°¸ì¡° í´ë˜ìŠ¤
        ]
        
        for selector in ref_selectors:
            ref_links = element.select(selector)
            for link in ref_links:
                ref_text = link.get_text(strip=True)
                href = link.get('href', '')
                if ref_text and href:
                    references.append(f"{ref_text}|{href}")
        
        return list(set(references))  # ì¤‘ë³µ ì œê±°
    
    def _extract_images_from_element(self, element) -> List[str]:
        """ìš”ì†Œì—ì„œ ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ (í™•ì¥ëœ ë²„ì „)"""
        images = []
        
        # 1. ì¼ë°˜ ì´ë¯¸ì§€ ë§í¬
        img_links = element.find_all('a', class_=lambda x: x and 'graphic' in x)
        for link in img_links:
            href = link.get('href', '')
            if href and 'image' in href:
                images.append(href)
        
        # 2. ì¸ë¼ì¸ ê·¸ë˜í”½ í…œí”Œë¦¿ 
        templates = element.find_next_siblings('template', limit=3)  # ë‹¤ìŒ 3ê°œê¹Œì§€ë§Œ í™•ì¸
        for template in templates:
            url = template.get('url', '')
            if url and 'image' in url:
                images.append(url)
        
        # 3. data ì†ì„±ì˜ ì´ë¯¸ì§€ ì •ë³´
        data_imgs = element.find_all(attrs={'data-inline-graphics': True})
        for img_elem in data_imgs:
            graphic_id = img_elem.get('data-inline-graphics')
            if graphic_id:
                # UpToDate ì´ë¯¸ì§€ URL íŒ¨í„´ ìƒì„±
                img_url = f"/contents/image?imageKey={graphic_id}"
                images.append(img_url)
        
        return list(set(images))  # ì¤‘ë³µ ì œê±°
    
    def _extract_medical_entities(self, text: str) -> List[str]:
        """ì˜ë£Œ ì—”í‹°í‹° ì¶”ì¶œ (í™•ì¥ëœ íŒ¨í„´)"""
        entities = []
        
        # ì˜ë£Œ ìš©ì–´ íŒ¨í„´ë“¤ (ë” í¬ê´„ì  - cost-effectiveness ë¬¸ì„œ íŠ¹í™”)
        patterns = [
            # ìˆ˜ìˆ /ì‹œìˆ /ì¹˜ë£Œ
            r'\b(?:pneumonectomy|lobectomy|thoracotomy|thoracoscopy|resection|surgery|procedure|intervention|treatment|therapy|operation)\b',
            
            # ì§ˆí™˜/í•©ë³‘ì¦
            r'\b(?:empyema|edema|fistula|syndrome|carcinoma|tumor|cancer|infection|hemorrhage|disease|disorder|condition|complication|morbidity|mortality)\b',
            
            # ì¸¡ì •/ê²€ì‚¬/ë¶„ì„
            r'\b(?:FEV1|DLCO|PPS|ARDS|CT|MRI|chest\s+radiograph|echocardiography|analysis|assessment|evaluation|study|trial|test|screening|examination)\b',
            
            # ì‹¬í˜ˆê´€ ê´€ë ¨
            r'\b(?:atrial fibrillation|myocardial infarction|pulmonary embolism|arrhythmia|tachycardia|cardiovascular|cardiac)\b',
            
            # í•´ë¶€í•™ì  êµ¬ì¡°
            r'\b(?:lung|bronchus|pleura|mediastinum|diaphragm|pericardium|ventricle|organ|tissue|anatomical)\b',
            
            # ê²½ì œ/ë¹„ìš© ë¶„ì„ ìš©ì–´ (cost-effectiveness íŠ¹í™”)
            r'\b(?:cost-effectiveness|cost-utility|cost-benefit|QALY|DALY|evLYG|GRA-QALY|economic|financial|budget|expenditure|expense|price|outcome|benefit|utility|value)\b',
            
            # ì—°êµ¬/ë°©ë²•ë¡ 
            r'\b(?:randomized|controlled|trial|study|meta-analysis|systematic\s+review|cohort|case-control|observational|prospective|retrospective|evidence|research)\b',
            
            # í†µê³„/ìˆ˜ì¹˜/ì§€í‘œ
            r'\b(?:mortality\s+rate|morbidity\s+rate|survival\s+rate|risk\s+ratio|odds\s+ratio|hazard\s+ratio|confidence\s+interval|p-value|statistics|probability|percentage)\b',
            
            # ì‹œê°„/ê¸°ê°„
            r'\b(?:life\s+expectancy|survival|follow-up|time\s+horizon|discount\s+rate|long-term|short-term|annual|yearly|monthly)\b',
            
            # ë‹¨ìœ„ê°€ ìˆëŠ” ìˆ˜ì¹˜
            r'\b\d+\s*(?:percent|%|mg|mL|years?|months?|days?|hours?|minutes?|dollars?|\$|USD|EUR)\b',
            
            # ì•½ë¬¼/ì˜ë£Œê¸°ê¸°
            r'\b(?:amiodarone|sotalol|antibiotics|analgesics|steroids|medication|drug|pharmaceutical|device|implant|prosthesis)\b',
            
            # ì˜ë£Œ ì „ë¬¸ ìš©ì–´
            r'\b(?:diagnosis|prognosis|etiology|pathophysiology|epidemiology|prevalence|incidence|sensitivity|specificity|efficacy|effectiveness|safety)\b'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            # ë§¤ì¹˜ ê²°ê³¼ ì²˜ë¦¬ - íŠœí”Œì´ë©´ í•©ì¹˜ê³ , ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ
            for match in matches:
                if isinstance(match, tuple):
                    entity = ' '.join(match).lower().strip()
                else:
                    entity = match.lower().strip()
                
                if entity and len(entity) > 1:
                    entities.append(entity)
        
        # ì¶”ê°€ì ìœ¼ë¡œ ì˜ë£Œ/ê²½ì œ ìš©ì–´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì¶”ì¶œ
        medical_keywords = [
            'patient', 'clinical', 'medical', 'health', 'healthcare', 'hospital', 'physician', 'doctor', 'nurse',
            'quality', 'safety', 'risk', 'benefit', 'outcome', 'result', 'effect', 'impact', 'improvement',
            'cost', 'economic', 'financial', 'budget', 'resource', 'allocation', 'efficiency', 'value',
            'analysis', 'evaluation', 'assessment', 'comparison', 'study', 'research', 'evidence',
            'mortality', 'morbidity', 'survival', 'life', 'death', 'disability', 'functional', 'status'
        ]
        
        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ í‚¤ì›Œë“œ ì°¾ê¸°
        words = re.findall(r'\b\w+\b', text.lower())
        for word in words:
            if word in medical_keywords and len(word) > 3:
                entities.append(word)
        
        # ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
        unique_entities = []
        seen = set()
        for entity in entities:
            entity_clean = entity.strip()
            if (len(entity_clean) > 2 and 
                entity_clean not in seen and 
                not entity_clean.isdigit() and  # ìˆœìˆ˜ ìˆ«ì ì œì™¸
                not all(c in '.,;:!?' for c in entity_clean)):  # êµ¬ë‘ì ë§Œ ìˆëŠ” ê²ƒ ì œì™¸
                unique_entities.append(entity_clean)
                seen.add(entity_clean)
        
        return unique_entities

    def get_parsing_stats(self) -> Dict:
        """íŒŒì‹± í†µê³„ ë°˜í™˜"""
        stats = {
            "document_hash": self.doc_hash,
            "total_sections": len(self.sections),
            "total_sentences": len(self.sentences),
            "sections_by_level": {},
            "sentences_by_section": {},
            "total_references": 0,
            "total_images": 0,
            "total_entities": 0
        }
        
        # ì„¹ì…˜ë³„ í†µê³„
        for section in self.sections:
            level = section.level
            stats["sections_by_level"][level] = stats["sections_by_level"].get(level, 0) + 1
        
        # ë¬¸ì¥ë³„ í†µê³„
        for sentence in self.sentences:
            section = sentence.section
            stats["sentences_by_section"][section] = stats["sentences_by_section"].get(section, 0) + 1
            stats["total_references"] += len(sentence.references)
            stats["total_images"] += len(sentence.images)
            stats["total_entities"] += len(sentence.medical_entities)
        
        return stats

    def export_parsed_data(self, output_path: str = None) -> Dict:
        """íŒŒì‹±ëœ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        data = {
            "document_info": {
                "hash": self.doc_hash,
                "parsing_stats": self.get_parsing_stats()
            },
            "sections": [
                {
                    "id": section.section_id,
                    "title": section.title,
                    "content": section.content,
                    "level": section.level
                }
                for section in self.sections
            ],
            "sentences": [
                {
                    "id": sentence.sentence_id,
                    "text": sentence.text,
                    "section": sentence.section,
                    "references": sentence.references,
                    "images": sentence.images,
                    "medical_entities": sentence.medical_entities
                }
                for sentence in self.sentences
            ]
        }
        
        if output_path:
            import json
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ Parsed data exported to {output_path}")
        
        return data

# í—¬í¼ í•¨ìˆ˜ë“¤
def load_uptodate_document(file_path: str) -> str:
    """UpToDate HTML ë¬¸ì„œ ë¡œë“œ"""
    print(f"ğŸ“‚ Loading document from {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    print(f"âœ… Document loaded ({len(content)} characters)")
    return content

def extract_document_info(html_content: str) -> Tuple[str, str]:
    """ë¬¸ì„œì—ì„œ ì œëª©ê³¼ URL ì¶”ì¶œ"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ì œëª© ì¶”ì¶œ
    title_div = soup.find('div', id='topicTitle')
    title = title_div.get_text(strip=True) if title_div else "Unknown Title"
    
    # URL ìƒì„± (ì‹¤ì œ URLì´ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì‚¬ìš©)
    url = f"https://www.uptodate.com/contents/{title.lower().replace(' ', '-').replace(',', '')}"
    
    print(f"ğŸ“‹ Document title: {title}")
    print(f"ğŸ”— Generated URL: {url}")
    
    return title, url

def parse_uptodate_file(file_path: str) -> MedicalDocumentParser:
    """UpToDate íŒŒì¼ì„ íŒŒì‹±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜"""
    # ë¬¸ì„œ ë¡œë“œ
    html_content = load_uptodate_document(file_path)
    title, url = extract_document_info(html_content)
    
    # íŒŒì„œ ìƒì„± ë° ì‹¤í–‰
    parser = MedicalDocumentParser(url)
    parser.parse_html(html_content)
    
    # í†µê³„ ì¶œë ¥
    stats = parser.get_parsing_stats()
    print(f"\nğŸ“Š Parsing Statistics:")
    print(f"  ğŸ“‘ Sections: {stats['total_sections']}")
    print(f"  ğŸ“ Sentences: {stats['total_sentences']}")
    print(f"  ğŸ“š References: {stats['total_references']}")
    print(f"  ğŸ–¼ï¸ Images: {stats['total_images']}")
    print(f"  ğŸ·ï¸ Medical Entities: {stats['total_entities']}")
    
    return parser

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import sys
    
    if len(sys.argv) > 1:
        file_path = sys.argv[1]
        parser = parse_uptodate_file(file_path)
        
        # JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
        output_file = file_path.replace('.html', '_parsed.json')
        parser.export_parsed_data(output_file)
    else:
        print("Usage: python medical_parser.py <uptodate_html_file>")